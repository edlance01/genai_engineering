{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91eb667",
   "metadata": {},
   "source": [
    "# Controlling LLM Output with Penalties and Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fd342",
   "metadata": {},
   "source": [
    "### Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a775bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the client (Replace 'your-api-key' or use environment variables)\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cae382",
   "metadata": {},
   "source": [
    "### Testing Function\n",
    "Allows different frequency and presence penalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt, max_tokens=100, freq_penalty=0.0, pres_penalty=0.0, temperature=0.8):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        frequency_penalty=freq_penalty,\n",
    "        presence_penalty=pres_penalty,\n",
    "        temperature=temperature,  # Slight randomness to see penalties in action\n",
    "    )\n",
    "    #return response.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af77dc",
   "metadata": {},
   "source": [
    "### Max Token Limits\n",
    "The max_tokens parameter is a hard stop. It doesn't tell the model to \"wrap it up\"; it literally cuts the model off mid-sentence if it reaches the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7799d274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Max Tokens: 20 (The 'Cliff-Hanger' Effect) ---\n",
      "The history of the Roman Empire is a captivating tale of growth, power, and eventual decline that spans\n",
      "Tokens Used: 41\n",
      "\n",
      "--- Max Tokens: 100 (The Standard Response) ---\n",
      "The history of the Roman Empire is a rich tapestry of power, innovation, and transformation, spanning several centuries and leaving an indelible mark on the world. It began with the end of the Roman Republic, which had been plagued by internal strife and civil wars. In 27 BC, Julius Caesar's adopted heir, Octavian, later known as Augustus, emerged victorious from the turmoil and was granted the title of the first emperor by the Roman Senate. This marked the beginning of the Pax Rom\n",
      "\n",
      "Input (Prompt) tokens: 21\n",
      "Output (Response) tokens: 100\n",
      "Total tokens used: 121\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a long, detailed paragraph about the history of the Roman Empire.\"\n",
    "\n",
    "# --- Test 1: Short Response ---\n",
    "print(\"--- Max Tokens: 20 (The 'Cliff-Hanger' Effect) ---\")\n",
    "response_short = get_llm_response(prompt, max_tokens=20)\n",
    "print(response_short.choices[0].message.content)  # Print the text\n",
    "print(f\"Tokens Used: {response_short.usage.total_tokens}\")\n",
    "\n",
    "print(\"\\n--- Max Tokens: 100 (The Standard Response) ---\")\n",
    "# --- Test 2: Longer Response ---\n",
    "response_long = get_llm_response(prompt, max_tokens=100)\n",
    "print(response_long.choices[0].message.content)  # Print the text\n",
    "\n",
    "# Displaying detailed breakdown\n",
    "print(f\"\\nInput (Prompt) tokens: {response_long.usage.prompt_tokens}\")\n",
    "print(f\"Output (Response) tokens: {response_long.usage.completion_tokens}\")\n",
    "print(f\"Total tokens used: {response_long.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be1077",
   "metadata": {},
   "source": [
    "### Frequency Penalty\n",
    "Frequency Penalty (Range: -2.0 to 2.0) penalizes tokens based on how many times they have already appeared in the text. The more a word is used, the less likely it is to be used again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a9a617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Frequency Penalty: 0.0 (Standard/Repetitive) ---\n",
      "ChatCompletion(id='chatcmpl-DC6YYmRRml6ngvl4Kw8w4rTBqfsFl', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Here are ten variations of saying \"hello\" using forms of the word:\\n\\n1. Hello!\\n2. Hello there!\\n3. Hello, hello!\\n4. Hellooo!\\n5. Hey, hello!\\n6. Well, hello!\\n7. Hello everyone!\\n8. Oh, hello!\\n9. Why, hello!\\n10. Hello, friend!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771777510, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=72, prompt_tokens=29, total_tokens=101, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Frequency Penalty: 2.0 (Forced Variety) ---\n",
      "ChatCompletion(id='chatcmpl-DC6YaZjcyqG1tMu1d7brhhi9lm25j', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Here are 10 different ways to say \"hello\" using variations of the word:\\n\\n1. Hello!\\n2. Hullo!\\n3. Halloo!\\n4. Hallo!\\n5. Hellooo?\\n6. Heeelllooo!!\\n7. \\'Ello!\\n8. Oh, hello.\\n9. Why, hello there.\\n10.Hello-hello!\\n\\nThese variations can convey different tones and contexts while still using the basic form of \"hello.\"', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771777512, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=93, prompt_tokens=29, total_tokens=122, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# We'll use a prompt that usually causes repetition\n",
    "prompt = (\n",
    "    \"List 10 ways to say 'hello' using only the word 'hello' and variations of 'hello'.\"\n",
    ")\n",
    "\n",
    "print(\"--- Frequency Penalty: 0.0 (Standard/Repetitive) ---\")\n",
    "print(get_llm_response(prompt, freq_penalty=0.0))\n",
    "\n",
    "print(\"\\n--- Frequency Penalty: 2.0 (Forced Variety) ---\")\n",
    "print(get_llm_response(prompt, freq_penalty=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e54e",
   "metadata": {},
   "source": [
    "### Presence Penalty\n",
    "Presence Penalty (Range: -2.0 to 2.0) penalizes a token if it has appeared at all so far. It doesn't care how many times it appeared; it just pushes the model to talk about new things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f6b468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Presence Penalty: 0.0 (Likely to stay on one point) ---\n",
      "ChatCompletion(id='chatcmpl-DC6n0104z5jneAbWLAT4iKSchYOWw', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Trees are vital to the environment as they produce oxygen through photosynthesis and absorb carbon dioxide, helping mitigate climate change. They provide habitat and food for a wide range of wildlife, enhancing biodiversity. Trees also contribute to human well-being by offering shade, reducing urban heat, and improving air quality. Additionally, they prevent soil erosion, conserve water, and can enhance the aesthetic value of landscapes, contributing to the overall health of ecosystems and communities.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778406, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=87, prompt_tokens=19, total_tokens=106, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Presence Penalty: 2.0 (Forced to switch to new sub-topics) ---\n",
      "ChatCompletion(id='chatcmpl-DC6n1erIn1TXOQ0wCGpEeGB20hjT8', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Trees are crucial for maintaining ecological balance as they absorb carbon dioxide and release oxygen, which is essential for the survival of most living organisms. They provide habitat and food for a wide range of species, contributing to biodiversity and ecosystem health. Trees also help in preventing soil erosion, conserving water, and moderating local climates by providing shade and reducing temperature extremes. Furthermore, trees enhance human well-being by improving air quality, offering recreational spaces, and adding aesthetic value to our surroundings.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778407, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=94, prompt_tokens=19, total_tokens=113, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# The prompt is designed to see if the model wanders off-topic\n",
    "prompt = \"Tell me about the importance of trees in 4 sentences.\"\n",
    "\n",
    "print(\"--- Presence Penalty: 0.0 (Likely to stay on one point) ---\")\n",
    "print(get_llm_response(prompt, pres_penalty=0.0))\n",
    "\n",
    "print(\"\\n--- Presence Penalty: 2.0 (Forced to switch to new sub-topics) ---\")\n",
    "print(get_llm_response(prompt, pres_penalty=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f73177",
   "metadata": {},
   "source": [
    "### Brainstorming Mode\n",
    "Combining high Presence Penalty with high Temperature creates the ultimate \"Brainstorming Mode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THE BRAINSTORMER (Temp 1.2, Presence 1.5) ---\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_llm_response() got an unexpected keyword argument 'temp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# High Temp (Creative) + High Presence (Diverse Topics)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- THE BRAINSTORMER (Temp 1.2, Presence 1.5) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_llm_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpres_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: get_llm_response() got an unexpected keyword argument 'temp'"
     ]
    }
   ],
   "source": [
    "# Scenario: Brainstorming a new sci-fi movie concept\n",
    "prompt = \"Give me a 5-sentence brainstorm for a unique sci-fi movie premise.\"\n",
    "\n",
    "# High Temp (Creative) + High Presence (Diverse Topics)\n",
    "print(\"--- THE BRAINSTORMER (Temp 1.2, Presence 1.5) ---\")\n",
    "print(get_llm_response(prompt, pres_penalty=1.5, max_tokens=150))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e46cc",
   "metadata": {},
   "source": [
    "### Comparison Summary Table\n",
    "\n",
    "| Parameter | Range | Primary Goal | Behavior |\n",
    "| :---------- | :---------- | :----------- | :----------- |\n",
    "| **Max Tokens** | 1 to 128k+ | **Length Control** | A \"Hard Stop\" that cuts the generation off at a specific token count. |\n",
    "| **Frequency Penalty** | -2.0 to 2.0 | **Anti-Repetition** | Penalties scale with **repetition count**. More uses = higher penalty. |\n",
    "| **Presence Penalty** | -2.0 to 2.0 | **Topic Diversity** | One-time penalty. If a word exists once, it gets penalized. |\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Guide\n",
    "\n",
    "| Goal | Parameter | Value |\n",
    "| :---------- | :---------- | :---------- |\n",
    "| **Stop cut-off sentences** | Max Tokens | Increase (e.g., 500) |\n",
    "| **Avoid repetitive words** | Frequency Penalty | 0.5 to 1.5 |\n",
    "| **Force new topics** | Presence Penalty | 0.5 to 1.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde1743",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
