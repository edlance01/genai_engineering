{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c568befc",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182869a",
   "metadata": {},
   "source": [
    "### Phase 1: Data Cleaning & Normalization\n",
    "The first step is to convert raw, noisy data into a uniform format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f220bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned: Check out our LLM tutorial Visit site Its very helpfull\n",
      "Normalized: check out our llm tutorial visit site its very helpfull\n"
     ]
    }
   ],
   "source": [
    "# --- \n",
    "# STEP 1: DATA CLEANING\n",
    "# Removing unwanted noise like HTML tags, irrelevant symbols, or ads[cite: 48, 55].\n",
    "# ---\n",
    "import re\n",
    "\n",
    "raw_data = \"Check out our LLM tutorial!!! <ads>Visit site</ads> It's very helpfull.\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags [cite: 52]\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove extra punctuation/noise [cite: 54, 70]\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "cleaned_data = clean_text(raw_data)\n",
    "print(f\"Cleaned: {cleaned_data}\")\n",
    "\n",
    "# ---\n",
    "# STEP 2: TEXT NORMALIZATION\n",
    "# Bringing uniformity by converting to lowercase to ensure consistency[cite: 66, 69].\n",
    "# ---\n",
    "normalized_data = cleaned_data.lower()\n",
    "print(f\"Normalized: {normalized_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d79ec72",
   "metadata": {},
   "source": [
    "### Phase 2: Tokenization & Numerical Conversion\n",
    "Before training, text must be broken down into units that the model can understand mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cfdc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['check', 'out', 'our', 'llm', 'tutorial', 'visit', 'site', 'its', 'very', 'helpfull']\n",
      "Numerical Mapping: {'very': 0, 'helpfull': 1, 'visit': 2, 'out': 3, 'our': 4, 'its': 5, 'tutorial': 6, 'check': 7, 'llm': 8, 'site': 9}\n",
      "Model Input: [7, 3, 4, 8, 6, 2, 9, 5, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/edwardlance/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# STEP 3: TOKENIZATION\n",
    "# Breaking sentences into smaller units (tokens) such as words or sub-words.\n",
    "# ---\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "tokens = word_tokenize(normalized_data)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# ---\n",
    "# STEP 4: NUMERICAL REPRESENTATION (Mock Example)\n",
    "# Converting tokens into numbers so the neural network can process them.\n",
    "# ---\n",
    "vocab = {word: i for i, word in enumerate(set(tokens))}\n",
    "numerical_form = [vocab[token] for token in tokens]\n",
    "\n",
    "print(f\"Numerical Mapping: {vocab}\")\n",
    "print(f\"Model Input: {numerical_form}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d04bae",
   "metadata": {},
   "source": [
    "### Phase 3: Dataset Preparation for Training\n",
    "Finally, the prepared data is split into different sets to test the model's actual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a89ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2\n",
      "Test set size: 1\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# STEP 5: DATA SPLITTING\n",
    "# Dividing the data into Training, Validation, and Test sets.\n",
    "# Training: For learning. Validation: For tuning. Test: For final evaluation.\n",
    "# ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mock dataset of cleaned sentences\n",
    "dataset = [\n",
    "    \"llm tutorial is helpful\",\n",
    "    \"data preparation is key\",\n",
    "    \"model training requires data\",\n",
    "]\n",
    "labels = [1, 1, 1]  # Example labels for supervised intent.\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Test set size: {len(test_data)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
