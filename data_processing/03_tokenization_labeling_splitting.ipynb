{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md62460615",
   "metadata": {},
   "source": [
    "# ðŸ““ Notebook 3 â€” Tokenization, Labeling & Data Splitting\n",
    "**LLM Data Processing Pipeline Â· Stage 3 of 3**\n",
    "\n",
    "The final preprocessing stage before model training:\n",
    "- Word-level and sub-word tokenization\n",
    "- Vocabulary building & token-to-ID mapping\n",
    "- Data labeling for supervised tasks\n",
    "- Train / Validation / Test splitting\n",
    "\n",
    "> **Prerequisites:** `pip install pandas scikit-learn` (tokenization is shown with pure Python first, then with `transformers` if available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md61876544",
   "metadata": {},
   "source": [
    "## 3.1 Setup & Normalized Input\n",
    "\n",
    "Simulated output from Notebook 2 â€” fully normalized text ready for tokenization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32611399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 10 sentences, 7 unique labels\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the quick brown fox jumped over the lazy dog</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>large language models learn from massive volum...</td>\n",
       "      <td>llm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural networks are the backbone of modern ai</td>\n",
       "      <td>neural_nets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformers changed natural language processi...</td>\n",
       "      <td>llm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deep learning models require large datasets fo...</td>\n",
       "      <td>deep_learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>do not underestimate the importance of data qu...</td>\n",
       "      <td>data_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>it is a great time to be working in machine le...</td>\n",
       "      <td>general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>data preparation is often the most time consum...</td>\n",
       "      <td>data_quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tokenization converts text into numerical repr...</td>\n",
       "      <td>tokenization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bias removal is essential for responsible ai s...</td>\n",
       "      <td>ethics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text          label\n",
       "0       the quick brown fox jumped over the lazy dog        general\n",
       "1  large language models learn from massive volum...            llm\n",
       "2      neural networks are the backbone of modern ai    neural_nets\n",
       "3  transformers changed natural language processi...            llm\n",
       "4  deep learning models require large datasets fo...  deep_learning\n",
       "5  do not underestimate the importance of data qu...   data_quality\n",
       "6  it is a great time to be working in machine le...        general\n",
       "7  data preparation is often the most time consum...   data_quality\n",
       "8  tokenization converts text into numerical repr...   tokenization\n",
       "9  bias removal is essential for responsible ai s...         ethics"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Simulated output from Notebook 2\n",
    "# -------------------------------------------------------------------\n",
    "sentences = [\n",
    "    \"the quick brown fox jumped over the lazy dog\",\n",
    "    \"large language models learn from massive volumes of text data\",\n",
    "    \"neural networks are the backbone of modern ai\",\n",
    "    \"transformers changed natural language processing forever\",\n",
    "    \"deep learning models require large datasets for training\",\n",
    "    \"do not underestimate the importance of data quality\",\n",
    "    \"it is a great time to be working in machine learning\",\n",
    "    \"data preparation is often the most time consuming step\",\n",
    "    \"tokenization converts text into numerical representations\",\n",
    "    \"bias removal is essential for responsible ai systems\",\n",
    "]\n",
    "\n",
    "labels = [\n",
    "    \"general\", \"llm\", \"neural_nets\", \"llm\", \"deep_learning\",\n",
    "    \"data_quality\", \"general\", \"data_quality\", \"tokenization\", \"ethics\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame({\"text\": sentences, \"label\": labels})\n",
    "print(f\"Corpus: {len(df)} sentences, {df['label'].nunique()} unique labels\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md70585070",
   "metadata": {},
   "source": [
    "## 3.2 Word-Level Tokenization\n",
    "\n",
    "The simplest tokenization: split on whitespace. Every unique word becomes a token.\n",
    "Word-level tokenizers struggle with unseen words (OOV â€” Out of Vocabulary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81073018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example tokenization:\n",
      "  [0] ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "  [1] ['large', 'language', 'models', 'learn', 'from', 'massive', 'volumes', 'of', 'text', 'data']\n",
      "\n",
      "Total tokens: 83\n",
      "Unique tokens (vocab size): 65\n",
      "Top 10 most frequent: [('the', 5), ('of', 3), ('data', 3), ('is', 3), ('large', 2), ('language', 2), ('models', 2), ('text', 2), ('ai', 2), ('learning', 2)]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Word tokenizer: split on whitespace / punctuation boundaries\n",
    "# -------------------------------------------------------------------\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "tokenized = [word_tokenize(sent) for sent in df[\"text\"]]\n",
    "\n",
    "print(\"Example tokenization:\")\n",
    "for i in range(2):\n",
    "    print(f\"  [{i}] {tokenized[i]}\")\n",
    "\n",
    "# Build vocabulary from training tokens\n",
    "all_tokens = [tok for sent_tokens in tokenized for tok in sent_tokens]\n",
    "vocab_counts = Counter(all_tokens)\n",
    "print(f\"\\nTotal tokens: {len(all_tokens)}\")\n",
    "print(f\"Unique tokens (vocab size): {len(vocab_counts)}\")\n",
    "print(f\"Top 10 most frequent: {vocab_counts.most_common(10)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md77030574",
   "metadata": {},
   "source": [
    "## 3.3 Building a Vocabulary & Token-to-ID Mapping\n",
    "\n",
    "Each unique token is assigned an integer ID. Special tokens\n",
    "`[PAD]`, `[UNK]`, `[CLS]`, `[SEP]` are reserved at the start of the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c91550708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (including special tokens): 70\n",
      "\n",
      "First 15 entries:\n",
      "    0  [PAD]\n",
      "    1  [UNK]\n",
      "    2  [CLS]\n",
      "    3  [SEP]\n",
      "    4  [MASK]\n",
      "    5  the\n",
      "    6  of\n",
      "    7  data\n",
      "    8  is\n",
      "    9  large\n",
      "   10  language\n",
      "   11  models\n",
      "   12  text\n",
      "   13  ai\n",
      "   14  learning\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Reserve special tokens, then assign IDs to corpus vocabulary\n",
    "# -------------------------------------------------------------------\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "# Sort by frequency (most common first) for stable vocab\n",
    "sorted_vocab = [tok for tok, _ in vocab_counts.most_common()]\n",
    "\n",
    "# Combine: special tokens first, then corpus tokens\n",
    "full_vocab = SPECIAL_TOKENS + sorted_vocab\n",
    "\n",
    "token2id = {tok: idx for idx, tok in enumerate(full_vocab)}\n",
    "id2token = {idx: tok for tok, idx in token2id.items()}\n",
    "\n",
    "print(f\"Vocabulary size (including special tokens): {len(token2id)}\")\n",
    "print(\"\\nFirst 15 entries:\")\n",
    "for tok, idx in list(token2id.items())[:15]:\n",
    "    print(f\"  {idx:3d}  {tok}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md20661140",
   "metadata": {},
   "source": [
    "## 3.4 Encoding Sentences to Integer Sequences\n",
    "\n",
    "Convert each sentence from a list of words to a list of integer IDs.\n",
    "Unknown tokens (not in vocab) map to the `[UNK]` ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81608058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded examples:\n",
      "  Sentence : the quick brown fox jumped over the lazy dog\n",
      "  Token IDs: [5, 17, 18, 19, 20, 21, 5, 22, 23]\n",
      "  Decoded  : ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "\n",
      "  Sentence : large language models learn from massive volumes of text data\n",
      "  Token IDs: [9, 10, 11, 24, 25, 26, 27, 6, 12, 7]\n",
      "  Decoded  : ['large', 'language', 'models', 'learn', 'from', 'massive', 'volumes', 'of', 'text', 'data']\n",
      "\n",
      "  Sentence : neural networks are the backbone of modern ai\n",
      "  Token IDs: [28, 29, 30, 5, 31, 6, 32, 13]\n",
      "  Decoded  : ['neural', 'networks', 'are', 'the', 'backbone', 'of', 'modern', 'ai']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Encode text â†’ integer IDs; unknown words â†’ [UNK]\n",
    "# -------------------------------------------------------------------\n",
    "UNK_ID = token2id[\"[UNK]\"]\n",
    "\n",
    "def encode(tokens, vocab):\n",
    "    return [vocab.get(tok, UNK_ID) for tok in tokens]\n",
    "\n",
    "def decode(ids, id_map):\n",
    "    return [id_map.get(i, \"[UNK]\") for i in ids]\n",
    "\n",
    "encoded = [encode(toks, token2id) for toks in tokenized]\n",
    "\n",
    "print(\"Encoded examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Sentence : {df['text'].iloc[i]}\")\n",
    "    print(f\"  Token IDs: {encoded[i]}\")\n",
    "    print(f\"  Decoded  : {decode(encoded[i], id2token)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md40236348",
   "metadata": {},
   "source": [
    "## 3.5 Sub-Word Tokenization (BPE concept)\n",
    "\n",
    "Real LLMs use sub-word tokenizers (BPE, WordPiece) to handle rare and OOV words.\n",
    "Here we demonstrate the concept with a tiny manual BPE-style merge step.\n",
    "For production, use `tokenizers` or `transformers` from HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38316759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace BERT WordPiece tokenization:\n",
      "  Input : 'tokenization'\n",
      "  Tokens: ['token', '##ization']\n",
      "  IDs   : [101, 19204, 3989, 102]\n",
      "\n",
      "  Input : 'preprocessing'\n",
      "  Tokens: ['prep', '##ro', '##ces', '##sing']\n",
      "  IDs   : [101, 17463, 3217, 9623, 7741, 102]\n",
      "\n",
      "  Input : 'unrelated'\n",
      "  Tokens: ['unrelated']\n",
      "  IDs   : [101, 15142, 102]\n",
      "\n",
      "  Input : 'LLMs learn from massive datasets'\n",
      "  Tokens: ['ll', '##ms', 'learn', 'from', 'massive', 'data', '##set', '##s']\n",
      "  IDs   : [101, 2222, 5244, 4553, 2013, 5294, 2951, 13462, 2015, 102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Minimal BPE demonstration: show how \"unrelated\" becomes\n",
    "# sub-word pieces rather than [UNK]\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Simulate HuggingFace tokenizer (works if transformers is installed)\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    examples = [\n",
    "        \"tokenization\",\n",
    "        \"preprocessing\",\n",
    "        \"unrelated\",\n",
    "        \"LLMs learn from massive datasets\",\n",
    "    ]\n",
    "    print(\"HuggingFace BERT WordPiece tokenization:\")\n",
    "    for ex in examples:\n",
    "        toks = tokenizer.tokenize(ex)\n",
    "        ids  = tokenizer.encode(ex, add_special_tokens=True)\n",
    "        print(f\"  Input : {ex!r}\")\n",
    "        print(f\"  Tokens: {toks}\")\n",
    "        print(f\"  IDs   : {ids}\")\n",
    "        print()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"transformers not installed â€” showing manual sub-word concept instead.\")\n",
    "    print()\n",
    "    # Manual character-pair demonstration\n",
    "    word = \"unrelated\"\n",
    "    chars = list(word)\n",
    "    print(f\"Word: {word!r}\")\n",
    "    print(f\"Character-level split: {chars}\")\n",
    "    print()\n",
    "    # In BPE, common pairs are merged. E.g. 'u','n' â†’ 'un', 'un','related' â†’ 'un-related'\n",
    "    print(\"BPE would progressively merge frequent pairs:\")\n",
    "    print(\"  Step 1: ['u','n','r','e','l','a','t','e','d']\")\n",
    "    print(\"  Step 2: ['un','r','e','l','a','t','e','d']   (merge 'u'+'n')\")\n",
    "    print(\"  Step 3: ['un','re','l','a','t','e','d']       (merge 'r'+'e')\")\n",
    "    print(\"  Step 4: ['un','relat','ed']                   (after more merges)\")\n",
    "    print(\"  â†’ Final sub-words map to known vocabulary IDs, no [UNK] needed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md60037639",
   "metadata": {},
   "source": [
    "## 3.6 Data Labeling\n",
    "\n",
    "For supervised tasks the model needs labels alongside input text.\n",
    "Here we show the label encoding step â€” converting string labels to integer class IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c14414408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping:\n",
      "  0  â†’  data_quality\n",
      "  1  â†’  deep_learning\n",
      "  2  â†’  ethics\n",
      "  3  â†’  general\n",
      "  4  â†’  llm\n",
      "  5  â†’  neural_nets\n",
      "  6  â†’  tokenization\n",
      "\n",
      "                                                text          label  label_id\n",
      "0       the quick brown fox jumped over the lazy dog        general         3\n",
      "1  large language models learn from massive volum...            llm         4\n",
      "2      neural networks are the backbone of modern ai    neural_nets         5\n",
      "3  transformers changed natural language processi...            llm         4\n",
      "4  deep learning models require large datasets fo...  deep_learning         1\n",
      "5  do not underestimate the importance of data qu...   data_quality         0\n",
      "6  it is a great time to be working in machine le...        general         3\n",
      "7  data preparation is often the most time consum...   data_quality         0\n",
      "8  tokenization converts text into numerical repr...   tokenization         6\n",
      "9  bias removal is essential for responsible ai s...         ethics         2\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Encode string labels â†’ integer class IDs\n",
    "# -------------------------------------------------------------------\n",
    "unique_labels = sorted(df[\"label\"].unique())\n",
    "label2id = {lbl: idx for idx, lbl in enumerate(unique_labels)}\n",
    "id2label = {idx: lbl for lbl, idx in label2id.items()}\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "print(\"Label mapping:\")\n",
    "for lbl, idx in label2id.items():\n",
    "    print(f\"  {idx}  â†’  {lbl}\")\n",
    "\n",
    "print()\n",
    "print(df[[\"text\", \"label\", \"label_id\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md88437979",
   "metadata": {},
   "source": [
    "## 3.7 Train / Validation / Test Split\n",
    "\n",
    "The prepared data is divided into three non-overlapping sets.\n",
    "We use stratified splitting to ensure each class is proportionally\n",
    "represented across all three sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c18780185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size     :    6  (60%)\n",
      "Validation size:    2  (20%)\n",
      "Test size      :    2  (20%)\n",
      "Total          : 10\n",
      "\n",
      "Train set:\n",
      "                                                    text        label\n",
      "     do not underestimate the importance of data quality data_quality\n",
      "    it is a great time to be working in machine learning      general\n",
      "  data preparation is often the most time consuming step data_quality\n",
      "    bias removal is essential for responsible ai systems       ethics\n",
      "           neural networks are the backbone of modern ai  neural_nets\n",
      "transformers changed natural language processing forever          llm\n",
      "\n",
      "Validation set:\n",
      "                                                    text         label\n",
      "            the quick brown fox jumped over the lazy dog       general\n",
      "deep learning models require large datasets for training deep_learning\n",
      "\n",
      "Test set:\n",
      "                                                         text        label\n",
      "    tokenization converts text into numerical representations tokenization\n",
      "large language models learn from massive volumes of text data          llm\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Stratified split: 70% train, 15% validation, 15% test\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # First split off test set\n",
    "    train_val, test = train_test_split(\n",
    "        df, test_size=0.15, random_state=42, stratify=df[\"label_id\"]\n",
    "        if df[\"label_id\"].value_counts().min() > 1 else None\n",
    "    )\n",
    "    # Then split validation from the remaining\n",
    "    train, val = train_test_split(\n",
    "        train_val, test_size=0.176, random_state=42  # 0.176 â‰ˆ 15% of total\n",
    "    )\n",
    "\n",
    "    print(f\"Train size     : {len(train):4d}  ({len(train)/len(df)*100:.0f}%)\")\n",
    "    print(f\"Validation size: {len(val):4d}  ({len(val)/len(df)*100:.0f}%)\")\n",
    "    print(f\"Test size      : {len(test):4d}  ({len(test)/len(df)*100:.0f}%)\")\n",
    "    print(f\"Total          : {len(train)+len(val)+len(test)}\")\n",
    "\n",
    "    print(\"\\nTrain set:\")\n",
    "    print(train[[\"text\",\"label\"]].to_string(index=False))\n",
    "    print(\"\\nValidation set:\")\n",
    "    print(val[[\"text\",\"label\"]].to_string(index=False))\n",
    "    print(\"\\nTest set:\")\n",
    "    print(test[[\"text\",\"label\"]].to_string(index=False))\n",
    "\n",
    "except ImportError:\n",
    "    print(\"scikit-learn not installed. Falling back to manual split.\")\n",
    "    n = len(df)\n",
    "    train = df.iloc[:int(n*0.70)]\n",
    "    val   = df.iloc[int(n*0.70):int(n*0.85)]\n",
    "    test  = df.iloc[int(n*0.85):]\n",
    "    print(f\"Train: {len(train)} | Val: {len(val)} | Test: {len(test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md36917486",
   "metadata": {},
   "source": [
    "## 3.8 Exporting the Preprocessed Dataset\n",
    "\n",
    "Save each split so it can be loaded by a training script without repeating all preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58068367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preprocessed_dataset.json\n",
      "  Vocab size : 70\n",
      "  Train rows : 6\n",
      "  Val rows   : 2\n",
      "  Test rows  : 2\n",
      "\n",
      "This file feeds directly into model training.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Save splits and vocabulary to JSON (works in any environment)\n",
    "# -------------------------------------------------------------------\n",
    "output = {\n",
    "    \"vocab\": token2id,\n",
    "    \"label_map\": label2id,\n",
    "    \"splits\": {\n",
    "        \"train\": train[[\"text\",\"label\",\"label_id\"]].to_dict(orient=\"records\"),\n",
    "        \"val\":   val[[\"text\",\"label\",\"label_id\"]].to_dict(orient=\"records\"),\n",
    "        \"test\":  test[[\"text\",\"label\",\"label_id\"]].to_dict(orient=\"records\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"preprocessed_dataset.json\", \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(\"Saved: preprocessed_dataset.json\")\n",
    "print(f\"  Vocab size : {len(token2id)}\")\n",
    "print(f\"  Train rows : {len(output['splits']['train'])}\")\n",
    "print(f\"  Val rows   : {len(output['splits']['val'])}\")\n",
    "print(f\"  Test rows  : {len(output['splits']['test'])}\")\n",
    "print()\n",
    "print(\"This file feeds directly into model training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md74155316",
   "metadata": {},
   "source": [
    "## 3.9 Summary â€” Full Pipeline Recap\n",
    "\n",
    "| Stage | Notebook | Key Steps |\n",
    "|-------|----------|-----------|\n",
    "| **Raw Data** | â€” | Web scrape, books, code repos, datasets |\n",
    "| **Data Cleaning** | 01 | Dedup, empty removal, noise removal, spell correction |\n",
    "| **Normalization** | 02 | Contractions, lowercase, punctuation, whitespace, bias flagging |\n",
    "| **Tokenization** | 03 | Word/sub-word split, vocab build, tokenâ†’ID encoding |\n",
    "| **Labeling** | 03 | String labels â†’ integer class IDs |\n",
    "| **Splitting** | 03 | 70% train / 15% val / 15% test |\n",
    "| **Training** | *next* | Feed `preprocessed_dataset.json` to model |\n",
    "\n",
    "> **Next step:** Load the saved JSON into your model training loop (PyTorch, TensorFlow, or HuggingFace `Trainer`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
