{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663a32f5",
   "metadata": {},
   "source": [
    "# Langchain Basic Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1a118b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple print output function for wrapping response\n",
    "def jprint(content, wrap_at=90):\n",
    "    \"\"\"Prints wrapped text or renders Markdown in Jupyter.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "\n",
    "        display(Markdown(content))\n",
    "    except ImportError:\n",
    "        import textwrap\n",
    "\n",
    "        print(textwrap.fill(content, width=wrap_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f9f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize a Chat Model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Summarize {topic} in one paragraph.\")\n",
    "\n",
    "# Initialize the parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Formatting the prompt with a variable\n",
    "# formatted_prompt = prompt.format(topic=\"Memory in AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fc445",
   "metadata": {},
   "source": [
    "### Creating a Basic Chain\n",
    "A Chain is a sequence of steps that process input one stage at a time. Each step transforms information before passing it forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ad3016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chains in LangChain are directed acyclic graphs that represent syntactic structures in natural language. Each node in a chain corresponds to a word or phrase in the sentence, and edges between nodes capture the relationships between them, such as dependencies or semantic roles. Chains provide a compact and structured way to represent linguistic information, making it useful for tasks such as parsing, semantic role labeling, and information extraction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chains turn reasoning into a structured pipeline\n",
    "# Here we combine: Prompt -> Model -> Output Parser\n",
    "chain = prompt | chat_model | parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"Chains in LangChain\"})\n",
    "jprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fc010",
   "metadata": {},
   "source": [
    "### Adding Memory\n",
    "Memory stores information from previous interactions. This enables continuity and context-aware conversations\n",
    "\n",
    "**NOTE - better to use langgraph now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35028465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: My name is Gemini\\nAI: Hello Gemini!'}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This is the old style, better to use Langgraph now!\n",
    "# Change the import to use the classic path\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"My name is Gemini\"}, {\"output\": \"Hello Gemini!\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66615578",
   "metadata": {},
   "source": [
    "### Tools and Agents\n",
    "Tools enable LLMs to take real actions like API calls or database queries. An Agent uses reasoning to decide which tool to use and when.\n",
    "\n",
    "In LangChain, an Agent acts as a reasoning engine. It doesn't just give one answer; it decides which Tool to use based on your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ad54719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Decision: Yes\n",
      "word in tool is: Antidisestablishmentarianism\n",
      "Tool Output: 28\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Initialize the LLM as a 'Reasoning Engine'\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# 2. Define a simple Tool (representing real actions)\n",
    "def get_word_length(word):\n",
    "    print(f\"word in tool is: {word}\")\n",
    "    return len(word)\n",
    "\n",
    "\n",
    "# 3. Create a 'Decision Logic' Prompt\n",
    "# Prompts are reusable, modular components\n",
    "logic_prompt = PromptTemplate.from_template(\n",
    "    \"You are an AI that decides if a tool is needed. \"\n",
    "    \"Tool: 'WordLength' (counts letters). \"\n",
    "    \"Question: {question}. \"\n",
    "    \"Do you need the tool? Answer 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "# 4. Building the Chain (a sequence of steps)\n",
    "# This connects the prompt, model, and output parser\n",
    "chain = logic_prompt | model | StrOutputParser()\n",
    "\n",
    "# 5. Execute and Observe [cite: 50]\n",
    "# question = \"How many letters are in all forms of the word 'Orchestration'?\"\n",
    "question = \"What is the exact letter count in the 3rd syllable of the word 'Antidisestablishmentarianism'?\"\n",
    "decision = chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Agent Decision: {decision}\")\n",
    "\n",
    "\n",
    "if \"Yes\" in decision:\n",
    "    # The app calls the tool to produce a structured decision\n",
    "    # result = get_word_length(\"Orchestration\")\n",
    "    result = get_word_length(\"Antidisestablishmentarianism\")\n",
    "    print(f\"Tool Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be00cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
