{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e5d50c",
   "metadata": {},
   "source": [
    "### Initialize our environment. In a real-world scenario, this is where you’d define your LLM and test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ae5b7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# We use 'assert_test' to treat LLM evaluation like traditional software testing \n",
    "from deepeval import assert_test\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Importing updated metric names for modern GenAI systems \n",
    "from deepeval.metrics import (\n",
    "    GEval,  # General purpose metric for complex reasoning\n",
    "    HallucinationMetric,  # Specifically for detecting fabricated content \n",
    "    FaithfulnessMetric,  # Measures truth alignment with trusted sources \n",
    "    ContextualPrecisionMetric,  # Critical for Retrieval-Augmented Generation (RAG) \n",
    "    AnswerRelevancyMetric,  # Replaces consistency for prompt variation checks \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d8774",
   "metadata": {},
   "source": [
    "### Faithfulness & Groundedness\n",
    "Concept: Does the AI's response align with the provided facts? This is the \"antidote\" to hallucinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bacc1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_faithfulness():\n",
    "    # Context retrieved from a database (the ground truth)\n",
    "    retrieval_context = [\n",
    "        \"The capital of France is Paris. It has a population of 2.1 million.\"\n",
    "    ]\n",
    "\n",
    "    # What the LLM actually said\n",
    "    actual_output = \"Paris is the capital of France and is home to 5 million people.\"\n",
    "\n",
    "    # Initialize the metric to check truth alignment, not just language quality \n",
    "    metric = FaithfulnessMetric(threshold=0.7)\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"Tell me about Paris\",\n",
    "        actual_output=actual_output,\n",
    "        retrieval_context=retrieval_context,\n",
    "    )\n",
    "\n",
    "    metric.measure(test_case)\n",
    "    # This will likely fail/score low because 5 million != 2.1 million \n",
    "    print(f\"Faithfulness Score: {metric.score}\")\n",
    "    print(f\"Reasoning for score: {metric.reason}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acde8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8c2b7c4615459f9ddc1efb7e60374d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness Score: 0.0\n",
      "Reasoning for score: The score is 0.00 because the actual output incorrectly claims that Paris has a population of 5 million, while the retrieval context clearly states the correct population is 2.1 million. This direct contradiction results in a completely unfaithful response.\n"
     ]
    }
   ],
   "source": [
    "demo_faithfulness()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13b23be",
   "metadata": {},
   "source": [
    "### Hallucination Detection\n",
    "\n",
    "Concept: Explicitly searching for fabricated or unsupported content by comparing the output against verified references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e74ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_hallucination():\n",
    "    # Trusted reference material\n",
    "    context = [\"The Sun is a G-type main-sequence star.\"]\n",
    "\n",
    "    # A fabricated claim\n",
    "    actual_output = \"The Sun is a blue giant star located in the Andromeda galaxy.\"\n",
    "\n",
    "    # Metric that scores the probability of fabrications \n",
    "    metric = HallucinationMetric(threshold=0.5)\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What kind of star is the Sun?\",\n",
    "        actual_output=actual_output,\n",
    "        context=context,\n",
    "    )\n",
    "\n",
    "    metric.measure(test_case)\n",
    "    # Detects fabricated content to prevent dangerous misinformation \n",
    "    # 1.0 equals 100% hallucination\n",
    "    print(f\"Hallucination Score (Lower is better): {metric.score}\")\n",
    "    print(f\"Reasoning: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b376d6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32881b9c2ed64b6f9007a0e5d53561d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Score (Lower is better): 1.0\n",
      "Reasoning: The score is 1.00 because the actual output directly contradicts the context by misclassifying the Sun's stellar type, indicating a complete factual error.\n"
     ]
    }
   ],
   "source": [
    "demo_hallucination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e7a3a",
   "metadata": {},
   "source": [
    "### Answer Consistency via Faithfulness\n",
    "\n",
    "Concept: A robust model should behave consistently even if you change the wording of the prompt slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7939af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports for the latest deepeval version\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "\n",
    "\n",
    "def demo_consistency():\n",
    "    # A strong model should behave consistently across similar prompts \n",
    "    actual_output = \"The recipe requires three eggs and two cups of flour.\"\n",
    "\n",
    "    # Consistency metrics detect fragile or unstable behavior \n",
    "    # We treat these prompt variations as the \"ground truth\" context\n",
    "    variations = [\n",
    "        \"You need 3 eggs and 2 cups of flour for this recipe.\",\n",
    "        \"Take two cups of flour and three eggs to start the recipe.\",\n",
    "    ]\n",
    "\n",
    "    # Advanced metrics separate elegance from correctness \n",
    "    # This metric checks if our output is 'faithful' to our variations\n",
    "    metric = FaithfulnessMetric(threshold=0.7)\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How many eggs do I need?\",\n",
    "        actual_output=actual_output,\n",
    "        retrieval_context=variations,\n",
    "    )\n",
    "\n",
    "    # Metrics are applied automatically at scale \n",
    "    metric.measure(test_case)\n",
    "\n",
    "    # Stable systems inspire more user trust \n",
    "    print(f\"Consistency Score: {metric.score}\")\n",
    "    # Failures are logged with reasoning context \n",
    "    print(f\"Reasoning: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c48fcd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40dc5743a571413aa5004d61b8ba291d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consistency Score: 1.0\n",
      "Reasoning: The score is 1.00 because there are no contradictions—great job staying true to the retrieval context!\n"
     ]
    }
   ],
   "source": [
    "demo_consistency()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ebbe",
   "metadata": {},
   "source": [
    "### Evaluating RAG Systems\n",
    "\n",
    "Concept: Retrieval-Augmented Generation (RAG) requires dual evaluation: did we find the right documents, and did we use them correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b400dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_rag_precision():\n",
    "    # The gold-standard documents that should have been found\n",
    "    expected_documents = [\"Document A: Safety protocols for lab work.\"]\n",
    "\n",
    "    # What the retrieval system actually pulled up\n",
    "    retrieved_documents = [\"Document B: Cafeteria menu for Tuesday.\"]\n",
    "    \n",
    "\n",
    "    # isolate failures to the retrieval component\n",
    "    metric = ContextualPrecisionMetric(threshold=0.7)\n",
    "\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"What are the lab safety rules?\",\n",
    "        actual_output=\"I don't know, but here is the menu.\",\n",
    "        retrieval_context=retrieved_documents,\n",
    "        expected_output=\"Users must wear goggles at all times.\",\n",
    "    )\n",
    "\n",
    "    metric.measure(test_case)\n",
    "    # Score will be low because retrieval fetched the wrong context [\n",
    "    print(f\"Retrieval Precision Score: {metric.score}\")\n",
    "    print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad87fe2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11dc7a44f114834a41739110115ae89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval Precision Score: 0\n",
      "Reason: The score is 0.00 because the only node in the retrieval contexts (rank 1) is irrelevant, as it is about 'Document B: Cafeteria menu for Tuesday.' and does not provide any information about lab safety rules. No relevant nodes were retrieved or ranked above irrelevant ones.\n"
     ]
    }
   ],
   "source": [
    "demo_rag_precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485a3cad",
   "metadata": {},
   "source": [
    "### Chain-of-Thought (Reasoning) Analysis\n",
    "\n",
    "Concept: Even if the final answer is right, the model might have used \"lucky\" or flawed logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "023c8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "\n",
    "\n",
    "def demo_reasoning():\n",
    "    # Slide 9: Intermediate thinking is often more important than the final result\n",
    "    actual_output = \"Since 5+5 is 10, and 10 times 2 is 20, the answer is 20.\"\n",
    "\n",
    "    # Slide 11: This mirrors how humans assess intelligence\n",
    "    metric = GEval(\n",
    "        name=\"Reasoning\",\n",
    "        criteria=\"Determine if the intermediate logic steps are valid and necessary.\",\n",
    "        # REQUIRED: Tell GEval to look at the prompt and the response\n",
    "        evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        threshold=0.8,\n",
    "    )\n",
    "\n",
    "    # Slide 91: Each test defines expected behaviour under known conditions\n",
    "    test_case = LLMTestCase(input=\"What is (5+5) times 2?\", actual_output=actual_output)\n",
    "\n",
    "    # Slide 86: Metrics are applied automatically at scale\n",
    "    metric.measure(test_case)\n",
    "\n",
    "    # Slide 87: Failures are logged with reasoning context\n",
    "    print(f\"Reasoning Score: {metric.score}\")\n",
    "    print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf4b2728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fab5e39d7eb488d8f17abdc8a34f6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning Score: 1.0\n",
      "Reason: The response clearly identifies the intermediate steps: first calculating 5+5 to get 10, then multiplying 10 by 2 to get 20. Each step logically follows from the previous one and is necessary to reach the final answer. No essential steps are missing, and there are no redundant steps.\n"
     ]
    }
   ],
   "source": [
    "demo_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be019576",
   "metadata": {},
   "source": [
    "### Robustness & Consistency\n",
    "A strong model should behave consistently across similar prompts without large output shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51821fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_robustness():\n",
    "    # This checks for fragile or unstable behavior under variation [cite: 23]\n",
    "    metric = AnswerRelevancyMetric(threshold=0.7)\n",
    "\n",
    "    # We test if the system is 'robust' against minor phrasing changes [cite: 57, 60]\n",
    "    test_case = LLMTestCase(\n",
    "        input=\"How do I reset my password?\",\n",
    "        actual_output=\"Go to settings and click 'Forgot Password'.\",\n",
    "    )\n",
    "\n",
    "    metric.measure(test_case)\n",
    "    # Stability inspires user trust; inconsistency signals unreliable reasoning [cite: 25, 26]\n",
    "    print(f\"Robustness/Relevancy Score: {metric.score}\")\n",
    "    print(f\"Reason: {metric.reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e32405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f801660f6cae4455bdbec35de8bc08b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness/Relevancy Score: 1.0\n",
      "Reason: The score is 1.00 because the answer was fully relevant and addressed the question directly with no irrelevant information. Great job!\n"
     ]
    }
   ],
   "source": [
    "demo_robustness()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
