{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57c7055",
   "metadata": {},
   "source": [
    "### Setup and Sample Dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b08f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Concept: Creating a test dataset\n",
    "data = {\n",
    "    \"prompt\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Explain photosynthesis in one sentence.\",\n",
    "        \"How do I reset my password?\",\n",
    "    ],\n",
    "    \"context\": [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"Photosynthesis is the process by which plants use sunlight to synthesize food.\",\n",
    "        \"Users can reset passwords via the 'Forgot Password' link on the login page.\",\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"Paris\",\n",
    "        \"Plants use sunlight to make food.\",\n",
    "        \"Use the 'Forgot Password' link.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2217a901",
   "metadata": {},
   "source": [
    "### Simulating Model Generation & Latency\n",
    "Demonstrates Model Generation and measures Latency, a key metric for user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a9bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_genai_call(prompt):\n",
    "    start_time = time.time()\n",
    "    # Simulating a \"varied\" response characteristic of GenAI \n",
    "    responses = {\n",
    "        \"What is the capital of France?\": \"The capital is Paris.\",\n",
    "        \"Explain photosynthesis in one sentence.\": \"Plants convert light into chemical energy.\",\n",
    "        \"How do I reset my password?\": \"Go to the settings and click reset.\"\n",
    "    }\n",
    "    time.sleep(random.uniform(0.5, 1.5)) # Simulating network/compute time \n",
    "    end_time = time.time()\n",
    "    return responses.get(prompt, \"I don't know.\"), (end_time - start_time)\n",
    "\n",
    "# Concept: Generation and Latency measurement [cite: 16, 109]\n",
    "results = [simulate_genai_call(p) for p in df['prompt']]\n",
    "df['model_output'] = [r[0] for r in results]\n",
    "df['latency_sec'] = [r[1] for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71482a33",
   "metadata": {},
   "source": [
    "### Automated Evaluation: Relevance & Faithfulness\n",
    "Here we simulate Automated Evaluation. \n",
    "We use a simple similarity check to mimic Relevance (matching intent) and Faithfulness (grounding in context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4adae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results Across Multiple Dimensions:\n",
      "                                    prompt  \\\n",
      "0           What is the capital of France?   \n",
      "1  Explain photosynthesis in one sentence.   \n",
      "2              How do I reset my password?   \n",
      "\n",
      "                                 model_output  relevance_score  \\\n",
      "0                       The capital is Paris.         0.384615   \n",
      "1  Plants convert light into chemical energy.         0.533333   \n",
      "2         Go to the settings and click reset.         0.303030   \n",
      "\n",
      "   faithfulness_score  \n",
      "0            0.807692  \n",
      "1            0.366667  \n",
      "2            0.236364  \n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "# This helper function calculates a similarity ratio between two strings (0.0 to 1.0).\n",
    "def calculate_score(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "\n",
    "# 1. Calculating Relevance:\n",
    "# Measures how well the model's output matches the \"Ground Truth\" (user intent)\n",
    "# Note ground proof might be 'Paris', other words are noise, thus the lower relevance score\n",
    "df[\"relevance_score\"] = df.apply(\n",
    "    lambda x: calculate_score(x[\"model_output\"], x[\"ground_truth\"]), axis=1\n",
    ")\n",
    "\n",
    "# 2. Calculating Faithfulness:\n",
    "# Measures if the response stays grounded in the provided \"Context\" without inventing info\n",
    "# Low score means LLM drifting in it's replies \n",
    "df[\"faithfulness_score\"] = df.apply(\n",
    "    lambda x: calculate_score(x[\"model_output\"], x[\"context\"]), axis=1\n",
    ")\n",
    "\n",
    "# Printing the analysis across multiple dimensions to see where the model succeeded or failed[cite: 18].\n",
    "print(\"Evaluation Results Across Multiple Dimensions:\")\n",
    "print(df[[\"prompt\", \"model_output\", \"relevance_score\", \"faithfulness_score\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301ccf9",
   "metadata": {},
   "source": [
    "### Demonstrating Robustness (Stability)\n",
    "\n",
    "Robustness measures how stable the model is when the prompt wording changes slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e004518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness Score (Stability): 0.24\n"
     ]
    }
   ],
   "source": [
    "# We define two versions of the same question to test stability\n",
    "prompt_v1 = \"What is the capital of France?\"\n",
    "prompt_v2 = \"Tell me the capital city of France.\"\n",
    "\n",
    "# We generate responses for both variations using our simulation function\n",
    "output_v1, _ = simulate_genai_call(prompt_v1)\n",
    "output_v2, _ = simulate_genai_call(prompt_v2)\n",
    "\n",
    "# Concept: Robustness Score\n",
    "# We compare the two outputs. If the model provides wildly different answers for\n",
    "# nearly identical questions, it indicates low stability\n",
    "robustness_score = calculate_score(output_v1, output_v2)\n",
    "print(f\"Robustness Score (Stability): {robustness_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90f940",
   "metadata": {},
   "source": [
    "### Human Evaluation Simulation\n",
    "Since automated metrics struggle with meaning , we add a column for Human Evaluation where a reviewer judges \"Clarity\" and \"Tone\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42645334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Areas identified for improvement:\n",
      "                        prompt               human_comments\n",
      "2  How do I reset my password?  Slightly vague instructions\n"
     ]
    }
   ],
   "source": [
    "# 1. Simulating Human Input:\n",
    "# We manually assign scores for 'Clarity', which is a subjective human judgment\n",
    "df[\"human_rating_clarity\"] = [5, 4, 3]  # Rating on a 1-5 scale.\n",
    "\n",
    "# 2. Capturing Nuance:\n",
    "# Humans provide qualitative feedback (comments) that machines cannot generate\n",
    "df[\"human_comments\"] = [\"Perfect\", \"A bit technical\", \"Slightly vague instructions\"]\n",
    "\n",
    "# 3. Identifying Weak Areas:\n",
    "# We filter the dataframe to find scores below 4.\n",
    "# This helps identify weak areas to be improved through tuning\n",
    "weak_areas = df[df[\"human_rating_clarity\"] < 4]\n",
    "\n",
    "print(\"Areas identified for improvement:\")\n",
    "print(weak_areas[[\"prompt\", \"human_comments\"]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
