{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cae2451",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) as a structured pipeline. \n",
    "\n",
    "The goal is to move from \"guessing\" to \"synthesizing verified information\" by making retrieval a natural transformation step in your code.\n",
    "\n",
    "In LCEL, this is achieved by transforming the user's raw input into a search query, fetching documents, and then piping those documents into a final prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0056e055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solid-state batteries are considered better because they use solid electrolytes instead of liquid ones, which offer higher energy density and improved safety over traditional lithium-ion batteries.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 1. THE BRAIN: Our LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# 2. THE DATA SOURCE: A mock retriever representing \"Verified Information\"\n",
    "# In a real app, this would be a Vector Database (Pinecone, Chroma, etc.)\n",
    "def mock_vector_db(query: str):\n",
    "    # This represents 'Truth becoming a structural outcome'\n",
    "    verified_context = (\n",
    "        \"Solid-state batteries use solid electrolytes instead of liquid ones. \"\n",
    "        \"They offer higher energy density and improved safety over lithium-ion.\"\n",
    "    )\n",
    "    return verified_context\n",
    "\n",
    "# 3. THE BLUEPRINT: A prompt that forces the LLM to use the context\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "# 4. THE PIPELINE: LCEL integrates retrieval as a natural transformation\n",
    "rag_chain = (\n",
    "    # Step 1: Prepare the inputs\n",
    "    # We pass the question through, and use the retriever to fetch context\n",
    "    {\"context\": mock_vector_db, \"question\": RunnablePassthrough()}\n",
    "    # Step 2: Feed into the prompt\n",
    "    | rag_prompt \n",
    "    # Step 3: Synthesis by the LLM using verified info\n",
    "    | llm \n",
    "    # Step 4: Clean string output\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 5. EXECUTION\n",
    "response = rag_chain.invoke(\"Why are solid-state batteries better?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
