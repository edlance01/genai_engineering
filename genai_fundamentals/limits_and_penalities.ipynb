{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91eb667",
   "metadata": {},
   "source": [
    "# Controlling LLM Output with Penalties and Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5fd342",
   "metadata": {},
   "source": [
    "### Setup and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a775bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the client (Replace 'your-api-key' or use environment variables)\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cae382",
   "metadata": {},
   "source": [
    "### Testing Function\n",
    "Allows different frequency and presence penalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7344fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt, max_tokens=100, freq_penalty=0.0, pres_penalty=0.0, temperature=0.8):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        frequency_penalty=freq_penalty,\n",
    "        presence_penalty=pres_penalty,\n",
    "        temperature=temperature,  # Slight randomness to see penalties in action\n",
    "    )\n",
    "    #return response.choices[0].message.content.strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af77dc",
   "metadata": {},
   "source": [
    "### Max Token Limits\n",
    "The max_tokens parameter is a hard stop. It doesn't tell the model to \"wrap it up\"; it literally cuts the model off mid-sentence if it reaches the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7799d274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Max Tokens: 20 (The 'Cliff-Hanger' Effect) ---\n",
      "The history of the Roman Empire is a grand narrative that chronicles the evolution of a small city-state into\n",
      "Tokens Used: 41\n",
      "\n",
      "--- Max Tokens: 100 (The Standard Response) ---\n",
      "The Roman Empire, one of history's most influential civilizations, emerged from the Roman Republic's decline in the first century BCE, marking a significant transformation in governance and territorial expansion. Its inception is traditionally dated to 27 BCE when Gaius Octavius, later known as Augustus, was granted imperium by the Senate, ushering in the era of the principate, a system that maintained republican forms while vesting substantial power in the emperor. Augustus implemented reforms that stabilized the empire after years of\n",
      "\n",
      "Input (Prompt) tokens: 21\n",
      "Output (Response) tokens: 100\n",
      "Total tokens used: 121\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a long, detailed paragraph about the history of the Roman Empire.\"\n",
    "\n",
    "# --- Test 1: Short Response ---\n",
    "print(\"--- Max Tokens: 20 (The 'Cliff-Hanger' Effect) ---\")\n",
    "response_short = get_llm_response(prompt, max_tokens=20)\n",
    "print(response_short.choices[0].message.content)  # Print the text\n",
    "print(f\"Tokens Used: {response_short.usage.total_tokens}\")\n",
    "\n",
    "print(\"\\n--- Max Tokens: 100 (The Standard Response) ---\")\n",
    "# --- Test 2: Longer Response ---\n",
    "response_long = get_llm_response(prompt, max_tokens=100)\n",
    "print(response_long.choices[0].message.content)  # Print the text\n",
    "\n",
    "# Displaying detailed breakdown\n",
    "print(f\"\\nInput (Prompt) tokens: {response_long.usage.prompt_tokens}\")\n",
    "print(f\"Output (Response) tokens: {response_long.usage.completion_tokens}\")\n",
    "print(f\"Total tokens used: {response_long.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be1077",
   "metadata": {},
   "source": [
    "### Frequency Penalty\n",
    "Frequency Penalty (Range: -2.0 to 2.0) penalizes tokens based on how many times they have already appeared in the text. The more a word is used, the less likely it is to be used again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9a617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Frequency Penalty: 0.0 (Standard/Repetitive) ---\n",
      "ChatCompletion(id='chatcmpl-DC6s8kVphDri0eWAUNKg0XIAUseSB', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Here are ten ways to say \"hello\" using variations of the word:\\n\\n1. Hello!\\n2. Hi!\\n3. Hey!\\n4. Hello there!\\n5. Hiya!\\n6. Hey there!\\n7. Heya!\\n8. Hello, hello!\\n9. Hi there!\\n10. Hellooo!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778724, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=65, prompt_tokens=29, total_tokens=94, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Frequency Penalty: 2.0 (Forced Variety) ---\n",
      "ChatCompletion(id='chatcmpl-DC6s9ZUE2qqVJ1Nek5ymxbyz4X5Hq', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Certainly! Here are ten ways to say \"hello\" using variations of the word:\\n\\n1. Hello!\\n2. Hellooo!\\n3. H-e-l-l-o?\\n4. Heeello!\\n5. Hallo!\\n6. Hi-llo!\\n7. Heeeey-looo!\\n8. Heylooo\\n9 . Hola-hello\\n10 . \\'Lo!\\n\\nThese variations can add different intonations or styles to a simple greeting!', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778725, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=90, prompt_tokens=29, total_tokens=119, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# We'll use a prompt that usually causes repetition\n",
    "prompt = (\n",
    "    \"List 10 ways to say 'hello' using only the word 'hello' and variations of 'hello'.\"\n",
    ")\n",
    "\n",
    "print(\"--- Frequency Penalty: 0.0 (Standard/Repetitive) ---\")\n",
    "print(get_llm_response(prompt, freq_penalty=0.0))\n",
    "\n",
    "print(\"\\n--- Frequency Penalty: 2.0 (Forced Variety) ---\")\n",
    "print(get_llm_response(prompt, freq_penalty=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994e54e",
   "metadata": {},
   "source": [
    "### Presence Penalty\n",
    "Presence Penalty (Range: -2.0 to 2.0) penalizes a token if it has appeared at all so far. It doesn't care how many times it appeared; it just pushes the model to talk about new things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f6b468c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Presence Penalty: 0.0 (Likely to stay on one point) ---\n",
      "ChatCompletion(id='chatcmpl-DC6sCJjGzi6B2rKGlw7YeRX4DpJZU', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Trees play a crucial role in maintaining ecological balance by absorbing carbon dioxide and releasing oxygen, which is essential for the survival of most life forms on Earth. Their roots help prevent soil erosion and maintain water cycles by facilitating groundwater recharge and reducing runoff. Additionally, trees provide habitats and food for countless species, supporting biodiversity and ecosystem health. Beyond environmental benefits, trees also offer economic value through resources like timber, fruits, and medicinal products, while enhancing human well-being by providing shade, reducing urban heat, and improving', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778728, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=100, prompt_tokens=19, total_tokens=119, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n",
      "\n",
      "--- Presence Penalty: 2.0 (Forced to switch to new sub-topics) ---\n",
      "ChatCompletion(id='chatcmpl-DC6sEk90yszL6tfd0BtwqNE0mOQgF', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Trees are essential for maintaining ecological balance as they produce oxygen through photosynthesis, which is vital for the survival of most life forms on Earth. They act as natural carbon sinks, absorbing carbon dioxide and helping to mitigate climate change. Trees also support biodiversity by providing habitat and food for numerous species, thereby sustaining complex ecosystems. Additionally, they offer shade, reduce urban heat, prevent soil erosion, and contribute to water conservation, enhancing both environmental quality and human well-being.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1771778730, model='gpt-4o-2024-08-06', object='chat.completion', service_tier='default', system_fingerprint='fp_01cbaa0587', usage=CompletionUsage(completion_tokens=92, prompt_tokens=19, total_tokens=111, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# The prompt is designed to see if the model wanders off-topic\n",
    "prompt = \"Tell me about the importance of trees in 4 sentences.\"\n",
    "\n",
    "print(\"--- Presence Penalty: 0.0 (Likely to stay on one point) ---\")\n",
    "print(get_llm_response(prompt, pres_penalty=0.0))\n",
    "\n",
    "print(\"\\n--- Presence Penalty: 2.0 (Forced to switch to new sub-topics) ---\")\n",
    "print(get_llm_response(prompt, pres_penalty=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f73177",
   "metadata": {},
   "source": [
    "### Brainstorming Mode\n",
    "Combining high Presence Penalty with high Temperature creates the ultimate \"Brainstorming Mode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0461e967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- THE BRAINSTORMER ---\n",
      "\n",
      "In a future where memories are traded as commodities, society is ravaged by\n",
      "identity theft and emotional detachment. Our protagonist, a disenchanted memory\n",
      "designer, stumbles upon an unaltered memory of Earth's forgotten distant past\n",
      "revealing the existence of intergalactic ancestors. This memory becomes highly\n",
      "sought after by factions that either wish to preserve human history or exploit\n",
      "forbidden knowledge for power. As minds merge in unforeseen ways, consciousness\n",
      "begins transcending physical form, challenging the very notion of humanity.\n",
      "Racing against time, the designer must decide whether restoring collective\n",
      "memory is worth potentially losing individuality forever.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# 1. Call the function (which returns the full object)\n",
    "response = get_llm_response(prompt, pres_penalty=1.5, max_tokens=150, temperature=1.2)\n",
    "\n",
    "# 2. Extract the string and convert it for textwrap right here\n",
    "response_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# 3. Now textwrap will work perfectly\n",
    "print(\"--- THE BRAINSTORMER ---\")\n",
    "print(\"\\n\" + textwrap.fill(response_text, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e46cc",
   "metadata": {},
   "source": [
    "### Comparison Summary Table\n",
    "\n",
    "| Parameter | Range | Primary Goal | Behavior |\n",
    "| :---------- | :---------- | :----------- | :----------- |\n",
    "| **Max Tokens** | 1 to 128k+ | **Length Control** | A \"Hard Stop\" that cuts the generation off at a specific token count. |\n",
    "| **Frequency Penalty** | -2.0 to 2.0 | **Anti-Repetition** | Penalties scale with **repetition count**. More uses = higher penalty. |\n",
    "| **Presence Penalty** | -2.0 to 2.0 | **Topic Diversity** | One-time penalty. If a word exists once, it gets penalized. |\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Guide\n",
    "\n",
    "| Goal | Parameter | Value |\n",
    "| :---------- | :---------- | :---------- |\n",
    "| **Stop cut-off sentences** | Max Tokens | Increase (e.g., 500) |\n",
    "| **Avoid repetitive words** | Frequency Penalty | 0.5 to 1.5 |\n",
    "| **Force new topics** | Presence Penalty | 0.5 to 1.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde1743",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
