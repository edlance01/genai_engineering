{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0217da",
   "metadata": {},
   "source": [
    "## Demo: Mastering LangChain Expression Language (LCEL)\n",
    "This series of demos is designed to accompany the Generative AI Engineering workshop. We will move beyond single LLM calls to explore Reasoning Dataflows.\n",
    "\n",
    "Key Objectives:\n",
    "* Runnables: Understanding the atomic unit of LCEL. (invoke, stream, batch)\n",
    "* The Pipe Operator (|): Visualizing the dataflow from prompt to parser.\n",
    "* RAG Integration: Connecting models to external knowledge streams.\n",
    "* Parallelism & Control: Implementing ensemble-style intelligence and cost-aware routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f577c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Model Initialized.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "# !pip install langchain langchain-openai\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "# Initialize the Model as a Transform Function\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "print(\"Setup Complete. Model Initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877066b2",
   "metadata": {},
   "source": [
    "### The Basic Pipe Flow\n",
    "The Pipe Flow Concept\n",
    "In LCEL, we treat reasoning as flowing data. Instead of nested function calls, we use the pipe operator (|) to create a clear, traceable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15523fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Output: Why did the AI engineer break up with their algorithm?\n",
      "\n",
      "Because they didn’t find it statistically significant anymore!\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the Prompt (Computational Object) [cite: 287]\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a brief professional joke about {topic}.\"\n",
    ")\n",
    "\n",
    "# 2. Define the Output Parser (Intelligence Filter) [cite: 301]\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 3. Construct the Chain using Pipe Flow\n",
    "# Flow: Input -> Prompt -> Model -> Parser\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Execute the pipeline\n",
    "response = chain.invoke({\"topic\": \"AI Engineering\"})\n",
    "print(f\"Chain Output: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7c86c4",
   "metadata": {},
   "source": [
    "### Parallel Reasoning Pipelines\n",
    "**Parallelism and Ensemble Intelligence:**\n",
    "Real-world problems often require multi-step reasoning. LCEL allows us to run multiple reasoning paths simultaneously—for example, generating an answer while a separate path critiques the reasoning or verifies facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f66daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel Result 1 (Joke): Why did OpenAI bring a ladder to the bar?\n",
      "\n",
      "Because it wanted to reach new heights in AI conversation!\n",
      "Parallel Result 2 (Fact): OpenAI was founded in December 2015 with the mission to ensure that artificial general intelligence (AGI) benefits all of humanity.\n"
     ]
    }
   ],
   "source": [
    "# Define two different reasoning paths\n",
    "joke_chain = ChatPromptTemplate.from_template(\"Joke about {topic}\") | model | parser\n",
    "fact_chain = (\n",
    "    ChatPromptTemplate.from_template(\"One real fact about {topic}\") | model | parser\n",
    ")\n",
    "\n",
    "# Combine them into a Parallel Pipeline\n",
    "map_chain = RunnableParallel(joke=joke_chain, fact=fact_chain)\n",
    "\n",
    "# Execute both simultaneously\n",
    "results = map_chain.invoke({\"topic\": \"OpenAI\"})\n",
    "print(f\"Parallel Result 1 (Joke): {results['joke']}\")\n",
    "print(f\"Parallel Result 2 (Fact): {results['fact']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a90301",
   "metadata": {},
   "source": [
    "### Cost-Aware Routing\n",
    "**Cost-Aware Execution:**\n",
    "Not every task requires the most powerful model. With LCEL, we can implement Conditional Logic to route simple tasks to smaller models (like GPT-4o-mini) and complex reasoning to larger ones, optimizing the system economically.\n",
    "\n",
    "RunnableBranch allows the pipeline to adapt dynamically, routing simple queries to efficient routes and complex tasks to deeper reasoning paths. This is essential for Cost-Aware Execution, ensuring intelligence is economically optimized by routing simpler steps to smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf281fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# 1. Define your \"Economy\" and \"Premium\" models\n",
    "economy_model = ChatOpenAI(model=\"gpt-4o-mini\")  # Fast and cheap\n",
    "premium_model = ChatOpenAI(model=\"gpt-4o\")  # High-reasoning and expensive\n",
    "\n",
    "# 2. Logic to extract the question and route\n",
    "# We use economy_model for simple paths to minimize price-per-token\n",
    "model_path_economy = (lambda x: x[\"question\"]) | economy_model\n",
    "model_path_premium = (lambda x: x[\"question\"]) | premium_model\n",
    "\n",
    "# 3. Cost-Aware Router\n",
    "branch = RunnableBranch(\n",
    "    # If the question is short/simple, use the cheap model\n",
    "    (lambda x: len(x[\"question\"]) < 50, model_path_economy),\n",
    "    # Only use the expensive model for long, complex tasks\n",
    "    model_path_premium,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
