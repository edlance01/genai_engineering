{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663a32f5",
   "metadata": {},
   "source": [
    "# Langchain Basic Orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a118b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple print output function for wrapping response\n",
    "def jprint(content, wrap_at=90):\n",
    "    \"\"\"Prints wrapped text or renders Markdown in Jupyter.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "\n",
    "        display(Markdown(content))\n",
    "    except ImportError:\n",
    "        import textwrap\n",
    "\n",
    "        print(textwrap.fill(content, width=wrap_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f9f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize a Chat Model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Summarize {topic} in one paragraph.\")\n",
    "\n",
    "# Initialize the parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Formatting the prompt with a variable\n",
    "# formatted_prompt = prompt.format(topic=\"Memory in AI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63fc445",
   "metadata": {},
   "source": [
    "### Creating a Basic Chain\n",
    "A Chain is a sequence of steps that process input one stage at a time. Each step transforms information before passing it forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ad3016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Chains in LangChain are a feature that allow users to create a series of automated actions that are triggered by certain events or conditions. These chains can be customized to perform a variety of tasks such as sending notifications, updating records, and executing external scripts. Users can define the specific conditions and actions for each step in the chain, making it a powerful tool for automating workflows and improving efficiency in language-related tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Chains turn reasoning into a structured pipeline\n",
    "# Here we combine: Prompt -> Model -> Output Parser\n",
    "chain = prompt | chat_model | parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"Chains in LangChain\"})\n",
    "jprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582fc010",
   "metadata": {},
   "source": [
    "### Adding Memory\n",
    "Memory stores information from previous interactions. This enables continuity and context-aware conversations\n",
    "\n",
    "**NOTE - better to use langgraph now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35028465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: My name is Gemini\\nAI: Hello Gemini!'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k5/rnrtn2ld6fsg4r3135y8rph40000gn/T/ipykernel_31561/4135528638.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This is the old style, better to use Langgraph now!\n",
    "# Change the import to use the classic path\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"My name is Gemini\"}, {\"output\": \"Hello Gemini!\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66615578",
   "metadata": {},
   "source": [
    "### Tools and Agents\n",
    "Tools enable LLMs to take real actions like API calls or database queries. An Agent uses reasoning to decide which tool to use and when.\n",
    "\n",
    "In LangChain, an Agent acts as a reasoning engine. It doesn't just give one answer; it decides which Tool to use based on your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ad54719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Decision: Yes\n",
      "Tool Output: The current time is 06:42 AM\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import datetime\n",
    "\n",
    "# 1. Initialize Model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "# 2. Define the Tool\n",
    "def get_current_time():\n",
    "    return datetime.datetime.now().strftime(\"%I:%M %p\")\n",
    "\n",
    "\n",
    "# 3. Decision Logic Prompt\n",
    "# We tell the AI it is disconnected from a live clock.\n",
    "logic_prompt = PromptTemplate.from_template(\n",
    "    \"You are an assistant without access to a real-time clock. \"\n",
    "    \"Tool Available: 'TimeCheck' (provides the current time). \"\n",
    "    \"Question: {question} \"\n",
    "    \"Do you need the tool to answer accurately? Answer 'Yes' or 'No'.\"\n",
    ")\n",
    "\n",
    "chain = logic_prompt | model | StrOutputParser()\n",
    "\n",
    "# 4. The Question\n",
    "question = \"What time is it right now?\"\n",
    "decision = chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Agent Decision: {decision}\")\n",
    "\n",
    "# 5. Conditional Tool Execution\n",
    "if \"Yes\" in decision:\n",
    "    result = get_current_time()\n",
    "    print(f\"Tool Output: The current time is {result}\")\n",
    "else:\n",
    "    print(\"The AI thought it knew the time... but it's likely hallucinating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be00cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
