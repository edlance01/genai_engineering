{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c1c6dc",
   "metadata": {},
   "source": [
    "# Langchain Foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2adebead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple print output function for wrapping response\n",
    "def jprint(content, wrap_at=90):\n",
    "    \"\"\"Prints wrapped text or renders Markdown in Jupyter.\"\"\"\n",
    "    try:\n",
    "        from IPython.display import display, Markdown\n",
    "\n",
    "        display(Markdown(content))\n",
    "    except ImportError:\n",
    "        import textwrap\n",
    "\n",
    "        print(textwrap.fill(content, width=wrap_at))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4b800",
   "metadata": {},
   "source": [
    "### Explaining Chat vs. Completion Models\n",
    "Direct API calls often return just plain text but LangChain provides a structured way to interact with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbad5a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain is a blockchain-based platform that aims to revolutionize the translation industry by connecting clients directly with professional translators. Through the use of blockchain technology, LangChain seeks to eliminate the need for intermediaries and reduce costs, while also ensuring transparency and security in translation transactions. This platform allows clients to easily find and hire professional translators for their projects, and provides a secure and efficient way for translators to receive payment for their services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize a Chat Model\n",
    "chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Sending a simple message\n",
    "# See langchain_core.messages (AI, Tool, System etc.)\n",
    "message = [HumanMessage(content=\"What is LangChain?\")]\n",
    "response = chat_model.invoke(message)\n",
    "\n",
    "jprint(response.content)  # LangChain connects LLMs with tools and data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2c1eb",
   "metadata": {},
   "source": [
    "### Reusable Prompts\n",
    "Instead of hard-coding text, LangChain uses Prompts as modular elements. They support dynamic variables to ensure consistent communication across workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd60777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Explain the concept of Memory in AI in one sentence."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Creating a reusable component\n",
    "# This allows us to use dynamic variables \n",
    "prompt = PromptTemplate.from_template(\"Explain the concept of {topic} in one sentence.\")\n",
    "\n",
    "# Formatting the prompt with a variable\n",
    "formatted_prompt = prompt.format(topic=\"Memory in AI\")\n",
    "\n",
    "jprint(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6454b",
   "metadata": {},
   "source": [
    "### Output Parsers\n",
    "LLMs alone generate raw text. Output Parsers bridge this by converting raw model output into structured formats reliable for application use.\n",
    "Think of it like a literal assembly line. Each piece of the chain implements a standard \"interface\" (the Runnable interface), which allows them to be snapped together.\n",
    "Input: You pass a prompt or message into the chain.\n",
    "  * Step 1 (chat_model): The model takes the input, calls the API, and outputs an AIMessage object.\n",
    "  * Step 2 (|): The pipe takes that AIMessage and automatically shoves it into the next component.\n",
    "  * Step 3 (parser): The parser takes the AIMessage, strips away the metadata, and returns just the clean string (or structured data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a913e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method StringPromptTemplate.pretty_print of PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Explain the concept of {topic} in one sentence.')>\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Intelligence is the ability to acquire and apply knowledge and skills to solve problems and adapt to new situations."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the parser\n",
    "# see langchain_core.output_parsers (json, yml, pydantic and many more)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# This ensures the response is clean and ready for program logic\n",
    "# chain = chat_model | parser # you will see it this way often, but there are gotchas, better to just add the prompt\n",
    "\n",
    "chain =  prompt | chat_model | parser\n",
    "result = chain.invoke({\"topic\": \"Intelligent\"})\n",
    "\n",
    "print(prompt.pretty_print)\n",
    "jprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
