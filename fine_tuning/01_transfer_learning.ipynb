{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m74096515",
   "metadata": {},
   "source": [
    "# Notebook 1 — Transfer Learning\n",
    "**PEFT & Transfer Learning Series · Part 1 of 3**\n",
    "\n",
    "Demonstrates all three Transfer Learning strategies:\n",
    "- **Feature Extraction** — freeze backbone, train new head only\n",
    "- **Fine-Tuning** — unfreeze some/all layers for deeper adaptation  \n",
    "- **Domain Adaptation** — same task, shifted data distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m82750672",
   "metadata": {},
   "source": [
    "## 1.1 Setup & Data\n",
    "\n",
    "We use a 20-feature, 4-class synthetic dataset representing pre-processed embeddings.\n",
    "A small neural network acts as our pre-trained base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39004220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([400, 20])  |  Test: torch.Size([100, 20])  |  Classes: 4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Synthetic dataset: 500 samples, 20 features, 4 classes\n",
    "# Think of these as sentence embeddings from a pre-trained encoder\n",
    "#\n",
    "# Starts as a 2D array of 500 rows × 20 columns. Each row is one \"sample\" (think: one sentence's embedding vector). \n",
    "# Each column is one feature (think: one dimension of that embedding):\n",
    "# -------------------------------------------------------------------\n",
    "X, y = make_classification(\n",
    "    n_samples=500, n_features=20, n_informative=10,\n",
    "    n_classes=4, n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "# After StandardScaler\n",
    "# Each of the 20 columns is rescaled so it has mean=0 and std=1. \n",
    "# The shape stays 500×20 but the values are now normalized:\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# After converting to PyTorch tensors (think 4D grid or larger)\n",
    "# The arrays become tensors with specific dtypes. The dtype matters because:\n",
    "\n",
    "# FloatTensor (float32) — required by the neural network's weight matrices\n",
    "# LongTensor (int64) — required by CrossEntropyLoss, which expects integer class indices\n",
    "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Xtr = torch.FloatTensor(Xtr); ytr = torch.LongTensor(ytr)\n",
    "Xte = torch.FloatTensor(Xte); yte = torch.LongTensor(yte)\n",
    "print(f'Train: {Xtr.shape}  |  Test: {Xte.shape}  |  Classes: 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m26272936",
   "metadata": {},
   "source": [
    "## 1.2 Define & Pre-Train the Base Model\n",
    "\n",
    "The backbone (layers 1–2) learns general representations.\n",
    "The head (layer 3) is task-specific and will be replaced per new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81696418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained base accuracy: 81.00%\n",
      "Total parameters: 7,716\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# backbone = shared feature extractor  |  head = task classifier\n",
    "# -------------------------------------------------------------------\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, in_features=20, hidden=64, num_classes=4):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),      nn.ReLU(),\n",
    "            nn.Linear(hidden, 32),          nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(32, num_classes)\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))\n",
    "\n",
    "def train(model, X, y, epochs=40, lr=1e-3):\n",
    "    opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    fn  = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        opt.zero_grad(); fn(model(X), y).backward(); opt.step()\n",
    "\n",
    "def acc(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return (model(X).argmax(1) == y).float().mean().item()\n",
    "\n",
    "pretrained = BaseModel()\n",
    "train(pretrained, Xtr, ytr, epochs=50)\n",
    "print(f'Pre-trained base accuracy: {acc(pretrained, Xte, yte):.2%}')\n",
    "print(f'Total parameters: {sum(p.numel() for p in pretrained.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m57727815",
   "metadata": {},
   "source": [
    "## 1.3 Feature Extraction (Transfer Learning)\n",
    "\n",
    "The backbone is **fully frozen** — no gradients flow through it.\n",
    "Only a fresh classification head is trained. Very fast; best when tasks are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35800851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 132 / 7,716  (1.7%)\n",
      "Feature extraction accuracy: 55.00%\n",
      "Backbone weights unchanged -- same backbone usable for other tasks.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Freeze all backbone parameters -- no gradients flow through them\n",
    "# -------------------------------------------------------------------\n",
    "feat_model = copy.deepcopy(pretrained)\n",
    "for param in feat_model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "feat_model.head = nn.Linear(32, 4)  # fresh head\n",
    "\n",
    "t = sum(p.numel() for p in feat_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in feat_model.parameters())\n",
    "print(f'Trainable: {t:,} / {n:,}  ({t/n:.1%})')\n",
    "\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, feat_model.parameters()), lr=1e-3)\n",
    "fn  = nn.CrossEntropyLoss()\n",
    "feat_model.train()\n",
    "for _ in range(40):\n",
    "    opt.zero_grad(); fn(feat_model(Xtr), ytr).backward(); opt.step()\n",
    "\n",
    "print(f'Feature extraction accuracy: {acc(feat_model, Xte, yte):.2%}')\n",
    "print('Backbone weights unchanged -- same backbone usable for other tasks.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m57629177",
   "metadata": {},
   "source": [
    "## 1.4 Fine-Tuning (Transfer Learning)\n",
    "\n",
    "We **unfreeze later backbone layers** so they adapt to the new task.\n",
    "A lower learning rate on pre-trained layers prevents catastrophic forgetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c35762251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: 6,372 / 7,716  (82.6%)\n",
      "Fine-tuned accuracy: 76.00%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Gradual unfreeze: keep early layers frozen, allow later ones to adapt\n",
    "# Differential LR: small for pre-trained layers, larger for new head\n",
    "# -------------------------------------------------------------------\n",
    "ft_model = copy.deepcopy(pretrained)\n",
    "ft_model.head = nn.Linear(32, 4)\n",
    "\n",
    "# Freeze first sub-layers (low-level general features)\n",
    "for i, layer in enumerate(ft_model.backbone):\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = (i >= 2)  # unfreeze from index 2 onward\n",
    "\n",
    "t = sum(p.numel() for p in ft_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in ft_model.parameters())\n",
    "print(f'Trainable: {t:,} / {n:,}  ({t/n:.1%})')\n",
    "\n",
    "opt = optim.Adam([\n",
    "    {'params': ft_model.backbone[2:].parameters(), 'lr': 2e-4},  # pre-trained: small LR\n",
    "    {'params': ft_model.head.parameters(),          'lr': 1e-3},  # head: larger LR\n",
    "])\n",
    "fn = nn.CrossEntropyLoss()\n",
    "ft_model.train()\n",
    "for _ in range(50):\n",
    "    opt.zero_grad(); fn(ft_model(Xtr), ytr).backward(); opt.step()\n",
    "\n",
    "print(f'Fine-tuned accuracy: {acc(ft_model, Xte, yte):.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m21073106",
   "metadata": {},
   "source": [
    "## 1.5 Domain Adaptation (Transfer Learning)\n",
    "\n",
    "The task stays the same but the data distribution shifts (e.g., news text to medical records).\n",
    "We simulate a domain shift, then adapt with only a small labeled sample from the new domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28434284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy BEFORE domain adaptation: 57.00%\n",
      "Accuracy AFTER  domain adaptation: 58.00%\n",
      "Improvement: +1.0pp  using only 20 adapted samples\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Simulate domain shift: add structured noise to test features\n",
    "# Adapt using only 20% of new-domain labeled data\n",
    "# -------------------------------------------------------------------\n",
    "X_nd = Xte.numpy() + np.random.normal(0, 1.5, Xte.shape)\n",
    "X_nd = torch.FloatTensor(StandardScaler().fit_transform(X_nd))\n",
    "y_nd = yte.clone()\n",
    "\n",
    "acc_before = acc(pretrained, X_nd, y_nd)\n",
    "print(f'Accuracy BEFORE domain adaptation: {acc_before:.2%}')\n",
    "\n",
    "adapted = copy.deepcopy(pretrained)\n",
    "adapted.head = nn.Linear(32, 4)\n",
    "for i, layer in enumerate(adapted.backbone):\n",
    "    for p in layer.parameters():\n",
    "        p.requires_grad = (i >= 4)  # only last block + head adapt\n",
    "\n",
    "n_adapt = int(len(X_nd) * 0.2)\n",
    "opt = optim.Adam(filter(lambda p: p.requires_grad, adapted.parameters()), lr=5e-4)\n",
    "fn  = nn.CrossEntropyLoss()\n",
    "adapted.train()\n",
    "for _ in range(60):\n",
    "    opt.zero_grad(); fn(adapted(X_nd[:n_adapt]), y_nd[:n_adapt]).backward(); opt.step()\n",
    "\n",
    "acc_after = acc(adapted, X_nd, y_nd)\n",
    "print(f'Accuracy AFTER  domain adaptation: {acc_after:.2%}')\n",
    "print(f'Improvement: +{(acc_after - acc_before)*100:.1f}pp  using only {n_adapt} adapted samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m47149108",
   "metadata": {},
   "source": [
    "## 1.6 Summary\n",
    "\n",
    "| Strategy | Backbone frozen? | Trainable % | Best when |\n",
    "|----------|-----------------|-------------|----------|\n",
    "| Feature Extraction | Fully | ~5% | New task similar to pre-training |\n",
    "| Fine-Tuning | Early layers | 40–60% | Enough data; more different task |\n",
    "| Domain Adaptation | Early layers | 30–50% | Same task, shifted distribution |\n",
    "\n",
    "**Key rule**: freeze early (general) layers; retrain late (task-specific) layers.\n",
    "\n",
    "Continue to **Notebook 2** for PEFT techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
