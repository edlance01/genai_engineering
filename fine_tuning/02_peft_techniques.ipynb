{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m13414759",
   "metadata": {},
   "source": [
    "# Notebook 2 — PEFT Techniques\n",
    "**PEFT & Transfer Learning Series · Part 2 of 3**\n",
    "\n",
    "Implements all four PEFT techniques from scratch in pure PyTorch.\n",
    "Each method keeps the base model frozen and adds only tiny trainable parameters.\n",
    "\n",
    "| Section | Technique | Core idea |\n",
    "|---------|-----------|----------|\n",
    "| 2.2 | Adapter Layers | Bottleneck modules inserted after each layer |\n",
    "| 2.3 | LoRA | Low-rank matrix pairs injected into weights |\n",
    "| 2.4 | Prompt Tuning | Learnable vectors prepended to input |\n",
    "| 2.5 | Prefix Tuning | Trainable vectors at every transformer layer |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m90414803",
   "metadata": {},
   "source": [
    "## 2.1 Base Model Setup\n",
    "\n",
    "A two-layer transformer-style model acts as our frozen pre-trained base.\n",
    "All PEFT methods use this same model — only the adapter strategy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c84620613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained accuracy: 12.50%\n",
      "Total base params: 28,740\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "D, VOCAB, SEQ, N_CLS, N = 32, 100, 10, 4, 200\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Tiny two-layer transformer (represents BERT / GPT-2 etc. at small scale)\n",
    "# -------------------------------------------------------------------\n",
    "class TinyLayer(nn.Module):\n",
    "    def __init__(self, d=D):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d, 4, batch_first=True)\n",
    "        self.ff   = nn.Sequential(nn.Linear(d, d*4), nn.GELU(), nn.Linear(d*4, d))\n",
    "        self.n1   = nn.LayerNorm(d); self.n2 = nn.LayerNorm(d)\n",
    "    def forward(self, x):\n",
    "        a, _ = self.attn(x, x, x)\n",
    "        x = self.n1(x + a); return self.n2(x + self.ff(x))\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed  = nn.Embedding(VOCAB, D)\n",
    "        self.layers = nn.ModuleList([TinyLayer() for _ in range(2)])\n",
    "        self.head   = nn.Linear(D, N_CLS)\n",
    "    def forward(self, x):\n",
    "        h = self.embed(x)\n",
    "        for l in self.layers: h = l(h)\n",
    "        return self.head(h.mean(1))\n",
    "\n",
    "X = torch.randint(0, VOCAB, (N, SEQ)); y = torch.randint(0, N_CLS, (N,))\n",
    "Xtr, Xte = X[:160], X[160:]; ytr, yte = y[:160], y[160:]\n",
    "\n",
    "def run(model, epochs=80, lr=1e-3):\n",
    "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    fn  = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        opt.zero_grad(); fn(model(Xtr), ytr).backward(); opt.step()\n",
    "\n",
    "def acc(model, X, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return (model(X).argmax(1) == y).float().mean().item()\n",
    "\n",
    "base = TinyModel()\n",
    "run(base, epochs=60)\n",
    "print(f'Pre-trained accuracy: {acc(base, Xte, yte):.2%}')\n",
    "print(f'Total base params: {sum(p.numel() for p in base.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m39603227",
   "metadata": {},
   "source": [
    "## 2.2 Adapter-Based Fine-Tuning\n",
    "\n",
    "A **bottleneck adapter** (down-project to rank r, activate, up-project) is inserted\n",
    "after each transformer layer. The up-projection is zero-initialized so the adapter\n",
    "starts as an identity function — safe to insert without disrupting the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37440542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter trainable: 584 / 29,324  (1.99%)\n",
      "Adapter accuracy : 12.50%\n",
      "Base accuracy    : 12.50%  (base unchanged)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Adapter: Linear(d->r) -> GELU -> Linear(r->d) + residual skip\n",
    "# Zero-init on up-proj = identity at init (no disruption to base model)\n",
    "# -------------------------------------------------------------------\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, d=D, r=4):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(d, r)\n",
    "        self.act  = nn.GELU()\n",
    "        self.up   = nn.Linear(r, d)\n",
    "        nn.init.zeros_(self.up.weight)  # zero-init: starts as identity\n",
    "        nn.init.zeros_(self.up.bias)\n",
    "    def forward(self, x):\n",
    "        return x + self.up(self.act(self.down(x)))  # residual\n",
    "\n",
    "class LayerWithAdapter(nn.Module):\n",
    "    def __init__(self, base_layer, r=4):\n",
    "        super().__init__()\n",
    "        self.base    = base_layer\n",
    "        self.adapter = Adapter(D, r)\n",
    "    def forward(self, x):\n",
    "        return self.adapter(self.base(x))\n",
    "\n",
    "adapter_model = copy.deepcopy(base)\n",
    "adapter_model.layers = nn.ModuleList([\n",
    "    LayerWithAdapter(layer) for layer in adapter_model.layers\n",
    "])\n",
    "# Freeze everything except adapter sub-modules\n",
    "for name, p in adapter_model.named_parameters():\n",
    "    p.requires_grad = 'adapter' in name\n",
    "\n",
    "t = sum(p.numel() for p in adapter_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in adapter_model.parameters())\n",
    "print(f'Adapter trainable: {t:,} / {n:,}  ({t/n:.2%})')\n",
    "\n",
    "run(adapter_model, epochs=80, lr=3e-3)\n",
    "print(f'Adapter accuracy : {acc(adapter_model, Xte, yte):.2%}')\n",
    "print(f'Base accuracy    : {acc(base, Xte, yte):.2%}  (base unchanged)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m79850735",
   "metadata": {},
   "source": [
    "## 2.3 LoRA — Low-Rank Adaptation\n",
    "\n",
    "LoRA decomposes the weight update into two small matrices: **delta_W = B @ A**\n",
    "- A is (r x d_in), B is (d_out x r), with r much smaller than d\n",
    "- B is zero-initialized so delta_W starts at zero — safe initialization\n",
    "- During training only A and B are updated; the frozen W never changes\n",
    "- At inference: `W_effective = W_frozen + (B @ A) * scale`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c68899106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA trainable : 1,280 / 30,020  (4.26%)\n",
      "A shape: torch.Size([4, 32])  (r x d_in)\n",
      "B shape: torch.Size([128, 4])  (d_out x r)\n",
      "LoRA accuracy  : 10.00%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# LoRALinear: wraps a frozen Linear, adds trainable low-rank delta\n",
    "# W_effective = W_frozen + B @ A * (alpha/r)\n",
    "# -------------------------------------------------------------------\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear, r=4, alpha=8):\n",
    "        super().__init__()\n",
    "        d_out, d_in = linear.weight.shape\n",
    "        self.frozen = linear\n",
    "        self.A = nn.Parameter(torch.randn(r, d_in) * 0.01)   # (r x d_in)\n",
    "        self.B = nn.Parameter(torch.zeros(d_out, r))           # (d_out x r) zero init\n",
    "        self.scale = alpha / r\n",
    "        self.frozen.weight.requires_grad = False\n",
    "        if self.frozen.bias is not None:\n",
    "            self.frozen.bias.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.frozen(x) + (x @ self.A.T @ self.B.T) * self.scale\n",
    "\n",
    "lora_model = copy.deepcopy(base)\n",
    "# Apply LoRA to the first FF linear in each layer\n",
    "for layer in lora_model.layers:\n",
    "    layer.ff[0] = LoRALinear(layer.ff[0], r=4, alpha=8)\n",
    "\n",
    "for name, p in lora_model.named_parameters():\n",
    "    p.requires_grad = ('.A' in name or '.B' in name)\n",
    "\n",
    "t = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in lora_model.parameters())\n",
    "print(f'LoRA trainable : {t:,} / {n:,}  ({t/n:.2%})')\n",
    "print(f'A shape: {lora_model.layers[0].ff[0].A.shape}  (r x d_in)')\n",
    "print(f'B shape: {lora_model.layers[0].ff[0].B.shape}  (d_out x r)')\n",
    "\n",
    "run(lora_model, epochs=80, lr=3e-3)\n",
    "print(f'LoRA accuracy  : {acc(lora_model, Xte, yte):.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m71048535",
   "metadata": {},
   "source": [
    "## 2.4 Prompt Tuning\n",
    "\n",
    "Learnable **soft prompt vectors** are prepended to the token embedding sequence\n",
    "before it enters the transformer. The entire base model stays frozen.\n",
    "Extremely lightweight — best for very large models where even LoRA is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46924091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning trainable: 160 / 28,900  (0.554%)\n",
      "Soft prompt shape: torch.Size([5, 32])  (n_virtual_tokens x d_model)\n",
      "Prompt tuning accuracy : 12.50%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Soft prompt: n_vt learnable embedding vectors prepended to real tokens\n",
    "# Only self.prompt is trained -- base model fully frozen\n",
    "# -------------------------------------------------------------------\n",
    "class PromptTunedModel(nn.Module):\n",
    "    def __init__(self, base, n_vt=5):\n",
    "        super().__init__()\n",
    "        self.base   = base\n",
    "        self.n_vt   = n_vt\n",
    "        self.prompt = nn.Parameter(torch.randn(n_vt, D) * 0.01)\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            tok_emb = self.base.embed(x)                                  # [B, seq, D]\n",
    "        prompt_exp = self.prompt.unsqueeze(0).expand(x.size(0), -1, -1)  # [B, n_vt, D]\n",
    "        h = torch.cat([prompt_exp, tok_emb], dim=1)                       # [B, n_vt+seq, D]\n",
    "        for layer in self.base.layers:\n",
    "            h = layer(h)\n",
    "        return self.base.head(h[:, self.n_vt:].mean(1))  # pool over real tokens\n",
    "\n",
    "pt_model = PromptTunedModel(copy.deepcopy(base), n_vt=5)\n",
    "for name, p in pt_model.named_parameters():\n",
    "    p.requires_grad = (name == 'prompt')\n",
    "\n",
    "t = sum(p.numel() for p in pt_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in pt_model.parameters())\n",
    "print(f'Prompt tuning trainable: {t:,} / {n:,}  ({t/n:.3%})')\n",
    "print(f'Soft prompt shape: {pt_model.prompt.shape}  (n_virtual_tokens x d_model)')\n",
    "\n",
    "run(pt_model, epochs=100, lr=1e-2)\n",
    "print(f'Prompt tuning accuracy : {acc(pt_model, Xte, yte):.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m67526798",
   "metadata": {},
   "source": [
    "## 2.5 Prefix Tuning\n",
    "\n",
    "Similar to prompt tuning but trainable prefix vectors are prepended at **every transformer layer**.\n",
    "This gives the prefix direct influence over each layer's attention computation,\n",
    "making it more expressive — especially effective for text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c36096701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix tuning trainable: 320 / 29,060  (1.101%)\n",
      "Prefix tuning accuracy : 10.00%\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# PrefixLayer: each transformer layer gets its own trainable prefix tokens\n",
    "# Prefix is concatenated to the full sequence for attention, then stripped from output\n",
    "# -------------------------------------------------------------------\n",
    "class PrefixLayer(nn.Module):\n",
    "    def __init__(self, base_layer, n_prefix=5):\n",
    "        super().__init__()\n",
    "        self.base     = base_layer\n",
    "        self.n_prefix = n_prefix\n",
    "        self.prefix   = nn.Parameter(torch.randn(n_prefix, D) * 0.01)\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        pre = self.prefix.unsqueeze(0).expand(B, -1, -1)  # [B, n_prefix, D]\n",
    "        x_ext = torch.cat([pre, x], dim=1)                 # [B, n_prefix+seq, D]\n",
    "        attn_out, _ = self.base.attn(x_ext, x_ext, x_ext)\n",
    "        attn_out = attn_out[:, self.n_prefix:]             # strip prefix from output\n",
    "        x = self.base.n1(x + attn_out)\n",
    "        return self.base.n2(x + self.base.ff(x))\n",
    "\n",
    "prefix_model = copy.deepcopy(base)\n",
    "prefix_model.layers = nn.ModuleList([\n",
    "    PrefixLayer(layer, n_prefix=5) for layer in prefix_model.layers\n",
    "])\n",
    "for name, p in prefix_model.named_parameters():\n",
    "    p.requires_grad = 'prefix' in name\n",
    "\n",
    "t = sum(p.numel() for p in prefix_model.parameters() if p.requires_grad)\n",
    "n = sum(p.numel() for p in prefix_model.parameters())\n",
    "print(f'Prefix tuning trainable: {t:,} / {n:,}  ({t/n:.3%})')\n",
    "\n",
    "run(prefix_model, epochs=100, lr=5e-3)\n",
    "print(f'Prefix tuning accuracy : {acc(prefix_model, Xte, yte):.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m87212465",
   "metadata": {},
   "source": [
    "## 2.6 Side-by-Side Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c11855408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method              Trainable  % of base   Accuracy\n",
      "----------------------------------------------------\n",
      "Full Fine-Tune         28,740    100.00%    10.00%\n",
      "Adapter                   584      2.03%    12.50%\n",
      "LoRA                    1,280      4.45%    10.00%\n",
      "Prompt Tuning             160      0.56%    12.50%\n",
      "Prefix Tuning             320      1.11%    10.00%\n"
     ]
    }
   ],
   "source": [
    "# Full fine-tune for reference\n",
    "full = copy.deepcopy(base)\n",
    "for p in full.parameters(): p.requires_grad = True\n",
    "run(full, epochs=80, lr=5e-4)\n",
    "\n",
    "base_total = sum(p.numel() for p in base.parameters())\n",
    "rows = [\n",
    "    ('Full Fine-Tune',  full,          base_total),\n",
    "    ('Adapter',         adapter_model, sum(p.numel() for p in adapter_model.parameters() if p.requires_grad)),\n",
    "    ('LoRA',            lora_model,    sum(p.numel() for p in lora_model.parameters()    if p.requires_grad)),\n",
    "    ('Prompt Tuning',   pt_model,      sum(p.numel() for p in pt_model.parameters()      if p.requires_grad)),\n",
    "    ('Prefix Tuning',   prefix_model,  sum(p.numel() for p in prefix_model.parameters() if p.requires_grad)),\n",
    "]\n",
    "print(f\"{'Method':<18} {'Trainable':>10} {'% of base':>10} {'Accuracy':>10}\")\n",
    "print('-' * 52)\n",
    "for name, model, t in rows:\n",
    "    print(f'{name:<18} {t:>10,} {t/base_total*100:>9.2f}% {acc(model, Xte, yte):>9.2%}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m55544142",
   "metadata": {},
   "source": [
    "## 2.7 Key Takeaways\n",
    "\n",
    "| Method | Params | Strength | Weakness |\n",
    "|--------|--------|----------|----------|\n",
    "| **Adapter** | ~5–10% | Easy per-task swapping | Adds inference latency |\n",
    "| **LoRA** | <1% | Industry standard; can merge into W | Must choose target modules |\n",
    "| **Prompt Tuning** | <0.1% | Extremely lightweight | Needs very large base model |\n",
    "| **Prefix Tuning** | <0.5% | More expressive than prompt tuning | Complex per-layer implementation |\n",
    "\n",
    "All four keep base weights frozen — the same base model can serve many tasks simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
