{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m35887763",
   "metadata": {},
   "source": [
    "# Notebook 3 — Comparison & Practical Workflow\n",
    "**PEFT & Transfer Learning Series · Part 3 of 3**\n",
    "\n",
    "This notebook provides:\n",
    "1. A unified benchmark comparing all strategies\n",
    "2. A decision function to pick the right strategy for your situation\n",
    "3. A save/load workflow for adapter weights\n",
    "4. HuggingFace PEFT integration (with graceful fallback)\n",
    "\n",
    "> Prerequisites: `pip install torch numpy` (optional: `transformers peft`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m19614002",
   "metadata": {},
   "source": [
    "## 3.1 Unified Benchmark Setup\n",
    "\n",
    "All strategies are measured consistently: trainable parameters, training time, accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57720001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model ready  |  28,740 total params\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import copy, time, json, os\n",
    "\n",
    "torch.manual_seed(42)\n",
    "D, VOCAB, SEQ, N_CLS, N = 32, 100, 10, 4, 200\n",
    "\n",
    "class TinyLayer(nn.Module):\n",
    "    def __init__(self, d=D):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d, 4, batch_first=True)\n",
    "        self.ff   = nn.Sequential(nn.Linear(d, d*4), nn.GELU(), nn.Linear(d*4, d))\n",
    "        self.n1   = nn.LayerNorm(d); self.n2 = nn.LayerNorm(d)\n",
    "    def forward(self, x):\n",
    "        a, _ = self.attn(x, x, x)\n",
    "        x = self.n1(x + a); return self.n2(x + self.ff(x))\n",
    "\n",
    "class TinyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed  = nn.Embedding(VOCAB, D)\n",
    "        self.layers = nn.ModuleList([TinyLayer() for _ in range(2)])\n",
    "        self.head   = nn.Linear(D, N_CLS)\n",
    "    def forward(self, x):\n",
    "        h = self.embed(x)\n",
    "        for l in self.layers: h = l(h)\n",
    "        return self.head(h.mean(1))\n",
    "\n",
    "X = torch.randint(0, VOCAB, (N, SEQ)); y = torch.randint(0, N_CLS, (N,))\n",
    "Xtr, Xte, ytr, yte = X[:160], X[160:], y[:160], y[160:]\n",
    "\n",
    "def benchmark(model, epochs=80, lr=1e-3, label=''):\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total     = sum(p.numel() for p in model.parameters())\n",
    "    opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    fn  = nn.CrossEntropyLoss()\n",
    "    t0 = time.time()\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        opt.zero_grad(); fn(model(Xtr), ytr).backward(); opt.step()\n",
    "    elapsed = time.time() - t0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = (model(Xte).argmax(1) == yte).float().mean().item()\n",
    "    return {'label':label, 'trainable':trainable, 'total':total,\n",
    "            'pct':trainable/total*100, 'acc':test_acc, 'time_s':elapsed}\n",
    "\n",
    "base = TinyModel()\n",
    "benchmark(base, epochs=60, label='pretrain')\n",
    "print(f'Base model ready  |  {sum(p.numel() for p in base.parameters()):,} total params')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m47445613",
   "metadata": {},
   "source": [
    "## 3.2 Run All Strategies\n",
    "\n",
    "Full fine-tune, Feature Extraction (TL), Adapter PEFT, and LoRA PEFT — all benchmarked on the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71806788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy                    Trainable      %   Accuracy   Time(s)\n",
      "-----------------------------------------------------------------\n",
      "Full Fine-Tune                 28,740 100.0%    17.50%     1.37s\n",
      "Feature Extraction (TL)           132   0.5%    15.00%     0.47s\n",
      "Adapter (PEFT)                    584   2.0%    17.50%     0.87s\n",
      "LoRA (PEFT)                     1,280   4.3%    20.00%     0.96s\n"
     ]
    }
   ],
   "source": [
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear, r=4, alpha=8):\n",
    "        super().__init__()\n",
    "        d_out, d_in = linear.weight.shape\n",
    "        self.frozen = linear\n",
    "        self.A = nn.Parameter(torch.randn(r, d_in) * 0.01)\n",
    "        self.B = nn.Parameter(torch.zeros(d_out, r))\n",
    "        self.scale = alpha / r\n",
    "        self.frozen.weight.requires_grad = False\n",
    "        if self.frozen.bias is not None: self.frozen.bias.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.frozen(x) + (x @ self.A.T @ self.B.T) * self.scale\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, d=D, r=4):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(d, r); self.act = nn.GELU(); self.up = nn.Linear(r, d)\n",
    "        nn.init.zeros_(self.up.weight); nn.init.zeros_(self.up.bias)\n",
    "    def forward(self, x): return x + self.up(self.act(self.down(x)))\n",
    "\n",
    "class LayerWithAdapter(nn.Module):\n",
    "    def __init__(self, base_layer):\n",
    "        super().__init__(); self.base = base_layer; self.adapter = Adapter()\n",
    "    def forward(self, x): return self.adapter(self.base(x))\n",
    "\n",
    "results = []\n",
    "\n",
    "# 1. Full fine-tune\n",
    "m = copy.deepcopy(base)\n",
    "for p in m.parameters(): p.requires_grad = True\n",
    "results.append(benchmark(m, epochs=80, lr=5e-4, label='Full Fine-Tune'))\n",
    "\n",
    "# 2. Feature extraction (Transfer Learning)\n",
    "m = copy.deepcopy(base); m.head = nn.Linear(D, N_CLS)\n",
    "for p in m.embed.parameters():  p.requires_grad = False\n",
    "for p in m.layers.parameters(): p.requires_grad = False\n",
    "results.append(benchmark(m, epochs=80, lr=1e-3, label='Feature Extraction (TL)'))\n",
    "\n",
    "# 3. Adapter PEFT\n",
    "m = copy.deepcopy(base)\n",
    "m.layers = nn.ModuleList([LayerWithAdapter(l) for l in m.layers])\n",
    "for name, p in m.named_parameters(): p.requires_grad = 'adapter' in name\n",
    "results.append(benchmark(m, epochs=80, lr=3e-3, label='Adapter (PEFT)'))\n",
    "\n",
    "# 4. LoRA PEFT\n",
    "m = copy.deepcopy(base)\n",
    "for layer in m.layers: layer.ff[0] = LoRALinear(layer.ff[0])\n",
    "for name, p in m.named_parameters(): p.requires_grad = ('.A' in name or '.B' in name)\n",
    "lora_trained = m\n",
    "results.append(benchmark(m, epochs=80, lr=3e-3, label='LoRA (PEFT)'))\n",
    "\n",
    "base_total = sum(p.numel() for p in base.parameters())\n",
    "print(f\"{'Strategy':<26} {'Trainable':>10} {'%':>6} {'Accuracy':>10} {'Time(s)':>9}\")\n",
    "print('-' * 65)\n",
    "for r in results:\n",
    "    print(f\"{r['label']:<26} {r['trainable']:>10,} {r['pct']:>5.1f}% {r['acc']:>9.2%} {r['time_s']:>8.2f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m12008763",
   "metadata": {},
   "source": [
    "## 3.3 Decision Framework\n",
    "\n",
    "Answer four questions to get the right strategy recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80660075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1: Running LLaMA-7B on a laptop\n",
      "Your situation:\n",
      "  Large model (>1B)  : True\n",
      "  Dataset size       : medium\n",
      "  Compute budget     : low\n",
      "  Task similar       : False\n",
      "\n",
      "  Recommendation : LoRA or Prompt Tuning\n",
      "  Reason         : Large model needs PEFT; low budget favors minimal params\n",
      "\n",
      "SCENARIO 2: BERT for a similar classification task, lots of data\n",
      "Your situation:\n",
      "  Large model (>1B)  : False\n",
      "  Dataset size       : large\n",
      "  Compute budget     : high\n",
      "  Task similar       : True\n",
      "\n",
      "  Recommendation : Full Fine-Tuning\n",
      "  Reason         : Resources allow it; maximizes task-specific accuracy\n",
      "\n",
      "SCENARIO 3: GPT-2 for niche domain, small dataset, tight budget\n",
      "Your situation:\n",
      "  Large model (>1B)  : False\n",
      "  Dataset size       : small\n",
      "  Compute budget     : low\n",
      "  Task similar       : False\n",
      "\n",
      "  Recommendation : Fine-Tuning with partial unfreeze\n",
      "  Reason         : Balances adaptation depth with overfitting risk\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Strategy recommender: answers based on your actual constraints\n",
    "# -------------------------------------------------------------------\n",
    "def recommend(large_model, data_size, compute, task_similar):\n",
    "    # large_model  : bool -- model larger than 1B params\n",
    "    # data_size    : str  -- 'small' / 'medium' / 'large'\n",
    "    # compute      : str  -- 'low' / 'medium' / 'high'\n",
    "    # task_similar : bool -- new task similar to pre-training domain\n",
    "    print('Your situation:')\n",
    "    print(f'  Large model (>1B)  : {large_model}')\n",
    "    print(f'  Dataset size       : {data_size}')\n",
    "    print(f'  Compute budget     : {compute}')\n",
    "    print(f'  Task similar       : {task_similar}')\n",
    "    print()\n",
    "    if large_model:\n",
    "        if compute == 'low':\n",
    "            rec = 'LoRA or Prompt Tuning'\n",
    "            why = 'Large model needs PEFT; low budget favors minimal params'\n",
    "        elif data_size == 'small':\n",
    "            rec = 'Prompt Tuning or Prefix Tuning'\n",
    "            why = 'Minimal params when labeled data is scarce'\n",
    "        else:\n",
    "            rec = 'LoRA (industry standard)'\n",
    "            why = 'Best accuracy-to-cost ratio for large LLMs'\n",
    "    else:\n",
    "        if task_similar and data_size in ('small', 'medium'):\n",
    "            rec = 'Feature Extraction (Transfer Learning)'\n",
    "            why = 'Frozen backbone already captures useful features; very fast'\n",
    "        elif compute == 'high' and data_size == 'large':\n",
    "            rec = 'Full Fine-Tuning'\n",
    "            why = 'Resources allow it; maximizes task-specific accuracy'\n",
    "        else:\n",
    "            rec = 'Fine-Tuning with partial unfreeze'\n",
    "            why = 'Balances adaptation depth with overfitting risk'\n",
    "    print(f'  Recommendation : {rec}')\n",
    "    print(f'  Reason         : {why}')\n",
    "\n",
    "print('SCENARIO 1: Running LLaMA-7B on a laptop')\n",
    "recommend(large_model=True,  data_size='medium', compute='low',    task_similar=False)\n",
    "print()\n",
    "print('SCENARIO 2: BERT for a similar classification task, lots of data')\n",
    "recommend(large_model=False, data_size='large',  compute='high',   task_similar=True)\n",
    "print()\n",
    "print('SCENARIO 3: GPT-2 for niche domain, small dataset, tight budget')\n",
    "recommend(large_model=False, data_size='small',  compute='low',    task_similar=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m21849800",
   "metadata": {},
   "source": [
    "## 3.4 Saving & Loading LoRA Adapter Weights\n",
    "\n",
    "One of PEFT's biggest practical wins: adapter files are tiny (megabytes, not gigabytes).\n",
    "You can version-control them, share on HuggingFace Hub, and swap tasks in seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c36288959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 tensors to /tmp/lora_demo/\n",
      "Loaded 4 tensors from /tmp/lora_demo/\n",
      "Accuracy after save/load round-trip: 20.00%  (original: 20.00%)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Save only the trainable LoRA A and B matrices\n",
    "# Load them back into a fresh base model with LoRA wrappers\n",
    "# -------------------------------------------------------------------\n",
    "def save_adapter(model, path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    state = {name: p.detach().cpu().tolist()\n",
    "             for name, p in model.named_parameters()\n",
    "             if '.A' in name or '.B' in name}\n",
    "    with open(f'{path}/adapter.json', 'w') as f:\n",
    "        json.dump(state, f)\n",
    "    config = {'method': 'lora', 'rank': 4, 'alpha': 8}\n",
    "    with open(f'{path}/config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f'Saved {len(state)} tensors to {path}/')\n",
    "\n",
    "def load_adapter(model, path):\n",
    "    with open(f'{path}/adapter.json') as f:\n",
    "        state = json.load(f)\n",
    "    sd = model.state_dict()\n",
    "    for name, val in state.items():\n",
    "        sd[name] = torch.tensor(val)\n",
    "    model.load_state_dict(sd)\n",
    "    print(f'Loaded {len(state)} tensors from {path}/')\n",
    "\n",
    "save_adapter(lora_trained, '/tmp/lora_demo')\n",
    "\n",
    "fresh = copy.deepcopy(base)\n",
    "for layer in fresh.layers:\n",
    "    layer.ff[0] = LoRALinear(layer.ff[0])\n",
    "load_adapter(fresh, '/tmp/lora_demo')\n",
    "\n",
    "fresh.eval()\n",
    "with torch.no_grad():\n",
    "    loaded_acc = (fresh(Xte).argmax(1) == yte).float().mean().item()\n",
    "orig_acc = next(r['acc'] for r in results if 'LoRA' in r['label'])\n",
    "print(f'Accuracy after save/load round-trip: {loaded_acc:.2%}  (original: {orig_acc:.2%})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m18261817",
   "metadata": {},
   "source": [
    "## 3.5 HuggingFace PEFT Integration\n",
    "\n",
    "In production, the `peft` library wraps any HuggingFace model with LoRA in 3 lines.\n",
    "This cell runs if `transformers` and `peft` are installed; otherwise it prints the equivalent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79019171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers / peft not installed.  Run:  pip install transformers peft\n",
      "\n",
      "Key LoraConfig parameters:\n",
      "  r              : rank of low-rank matrices (4-64; lower = fewer params)\n",
      "  lora_alpha     : scaling factor (usually 2x r)\n",
      "  target_modules : which weight matrices to apply LoRA to\n",
      "                   ['query','value'] is the standard BERT choice\n",
      "  lora_dropout   : regularization for LoRA layers\n",
      "\n",
      "After wrapping:\n",
      "  peft_model.save_pretrained(\"./adapter\")  # saves only adapter weights (~MB)\n",
      "  peft_model.push_to_hub(\"username/my-lora\")  # share on HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Production LoRA with HuggingFace peft library\n",
    "# Install: pip install transformers peft\n",
    "# -------------------------------------------------------------------\n",
    "try:\n",
    "    from transformers import AutoModelForSequenceClassification\n",
    "    from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'bert-base-uncased', num_labels=4\n",
    "    )\n",
    "    config = LoraConfig(\n",
    "        task_type      = TaskType.SEQ_CLS,\n",
    "        r              = 8,\n",
    "        lora_alpha     = 16,\n",
    "        lora_dropout   = 0.1,\n",
    "        target_modules = ['query', 'value'],\n",
    "    )\n",
    "    peft_model = get_peft_model(model, config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "    print('HuggingFace PEFT LoRA model ready.')\n",
    "\n",
    "except ImportError:\n",
    "    print('transformers / peft not installed.  Run:  pip install transformers peft')\n",
    "    print()\n",
    "    print('Key LoraConfig parameters:')\n",
    "    print('  r              : rank of low-rank matrices (4-64; lower = fewer params)')\n",
    "    print('  lora_alpha     : scaling factor (usually 2x r)')\n",
    "    print('  target_modules : which weight matrices to apply LoRA to')\n",
    "    print(\"                   ['query','value'] is the standard BERT choice\")\n",
    "    print('  lora_dropout   : regularization for LoRA layers')\n",
    "    print()\n",
    "    print('After wrapping:')\n",
    "    print('  peft_model.save_pretrained(\"./adapter\")  # saves only adapter weights (~MB)')\n",
    "    print('  peft_model.push_to_hub(\"username/my-lora\")  # share on HuggingFace Hub')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m22013406",
   "metadata": {},
   "source": [
    "## 3.6 Full Series Summary\n",
    "\n",
    "| | Full Fine-Tuning | PEFT (LoRA / Adapter) | Transfer Learning |\n",
    "|---|---|---|---|\n",
    "| **Params trained** | All | <1–10% | Head + optional layers |\n",
    "| **Memory** | High | Low | Low to medium |\n",
    "| **Data needed** | Large | Small | Small |\n",
    "| **Accuracy** | Highest | Near-equal | Good (similar tasks) |\n",
    "| **Concept** | Training strategy | Fine-tuning strategy | Learning paradigm |\n",
    "| **Best for** | Unlimited resources | Large models, tight budget | Related tasks, small data |\n",
    "\n",
    "**Core message**: PEFT and Transfer Learning make high-quality AI practical on commodity hardware —\n",
    "enabling chatbots, medical AI, and recommendation systems without multi-million-dollar compute budgets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
